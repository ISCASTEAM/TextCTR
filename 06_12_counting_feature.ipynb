{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "trusted": true,
    "collapsed": false,
    "id": "D5EA4CEFB552410F8644823F1A381B1F",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "from sklearn.model_selection import StratifiedKFold\r\n",
    "import nltk\r\n",
    "import pickle\r\n",
    "from nltk import pos_tag\r\n",
    "from scipy.sparse import hstack, vstack\r\n",
    "from scipy import spatial\r\n",
    "from sklearn.datasets import dump_svmlight_file,load_svmlight_file\r\n",
    "from sklearn.decomposition import TruncatedSVD\r\n",
    "import xgboost as xgb\r\n",
    "import gensim\r\n",
    "from gensim.models import TfidfModel\r\n",
    "from gensim.corpora import Dictionary\r\n",
    "import gc\r\n",
    "# 显示cell运行时长\r\n",
    "%load_ext klab-autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "trusted": true,
    "collapsed": false,
    "id": "8954AA8BC04149D18831EB32B9E2D034",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 672 µs\n"
     ]
    }
   ],
   "source": [
    "feature_path = \"/home/kesci/work/counting_feature/train/\"\n",
    "data_path = \"/home/kesci/input/bytedance/first-round/\"\n",
    "colnames = [\"query_id\",\"query\",\"query_title_id\",\"title\",\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "trusted": true,
    "collapsed": false,
    "id": "5DA383615E9A4C62B35BFA2D6F4C867A",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 19.8 ms\n"
     ]
    }
   ],
   "source": [
    "#  计算统计的相似度特征 (雅克比相似度,dice相似度)\r\n",
    "#  计算统计的距离-index-特征 (min,max,median,avg,std)\r\n",
    "\"\"\"\r\n",
    "以下特征 unigram,bigram,trigram\r\n",
    "\"\"\"\r\n",
    "def try_divide(x, y, val=0.0):\r\n",
    "    if y != 0.0:\r\n",
    "        val = float(x) / y\r\n",
    "    return val\r\n",
    "    \r\n",
    "def JaccardCoef(A, B):\r\n",
    "    A, B = set(A), set(B)\r\n",
    "    intersect = len(A.intersection(B))\r\n",
    "    union = len(A.union(B))\r\n",
    "    coef = try_divide(intersect, union)\r\n",
    "    return coef\r\n",
    "    \r\n",
    "def JaccardCoef_Join(A, B):\r\n",
    "    A, B = set(A), set(B)\r\n",
    "    return len(A.intersection(B))\r\n",
    "    \r\n",
    "def DiceDist(A, B):\r\n",
    "    A, B = set(A), set(B)\r\n",
    "    intersect = len(A.intersection(B))\r\n",
    "    union = len(A) + len(B)\r\n",
    "    d = try_divide(2*intersect, union)\r\n",
    "    return d\r\n",
    "\r\n",
    "def getUnigram(words):\r\n",
    "    \"\"\"\r\n",
    "        Input: a list of words, e.g., ['I', 'am', 'Denny']\r\n",
    "        Output: a list of unigram\r\n",
    "    \"\"\"\r\n",
    "    assert type(words) == list\r\n",
    "    return words\r\n",
    "\r\n",
    "def getBigram(words, join_string=\"_\", skip=0):\r\n",
    "    \"\"\"\r\n",
    "       Input: a list of words, e.g., ['I', 'am', 'Denny']\r\n",
    "       Output: a list of bigram, e.g., ['I_am', 'am_Denny']\r\n",
    "       I use _ as join_string for this example.\r\n",
    "    \"\"\"\r\n",
    "    assert type(words) == list\r\n",
    "    L = len(words)\r\n",
    "    if L > 1:\r\n",
    "        lst = []\r\n",
    "        for i in range(L-1):\r\n",
    "            for k in range(1,skip+2):\r\n",
    "                if i+k < L:\r\n",
    "                    lst.append( join_string.join([words[i], words[i+k]]) )\r\n",
    "    else:\r\n",
    "        # set it as unigram\r\n",
    "        lst = getUnigram(words)\r\n",
    "    return lst\r\n",
    "    \r\n",
    "def getTrigram(words, join_string=\"_\", skip=0):\r\n",
    "    \"\"\"\r\n",
    "       Input: a list of words, e.g., ['I', 'am', 'Denny']\r\n",
    "       Output: a list of trigram, e.g., ['I_am_Denny']\r\n",
    "       I use _ as join_string for this example.\r\n",
    "    \"\"\"\r\n",
    "    assert type(words) == list\r\n",
    "    L = len(words)\r\n",
    "    if L > 2:\r\n",
    "        lst = []\r\n",
    "        for i in range(L-2):\r\n",
    "            for k1 in range(1,skip+2):\r\n",
    "                for k2 in range(1,skip+2):\r\n",
    "                    if i+k1 < L and i+k1+k2 < L:\r\n",
    "                        lst.append( join_string.join([words[i], words[i+k1], words[i+k1+k2]]) )\r\n",
    "    else:\r\n",
    "        # set it as bigram\r\n",
    "        lst = getBigram(words, join_string, skip)\r\n",
    "    return lst\r\n",
    "\r\n",
    "def wc_ratio_intersect(A,B):\r\n",
    "    wc_A = 0\r\n",
    "    wc_B = 0\r\n",
    "    for a in A:\r\n",
    "        if a in B:\r\n",
    "            wc_A += 1\r\n",
    "    for b in B:\r\n",
    "        if b in A:\r\n",
    "            wc_B += 1\r\n",
    "    ratio_A = float(wc_A)/len(A)\r\n",
    "    ratio_B = float(wc_B)/len(B)\r\n",
    "    return wc_A,wc_B,ratio_A,ratio_B\r\n",
    "\r\n",
    "    \r\n",
    "def trigram_intersect_func(query,title):\r\n",
    "    my_list = []\r\n",
    "    # unigram\r\n",
    "    wc_unigram_q,wc_unigram_t,ratio_unigram_q,ratio_unigram_t = wc_ratio_intersect(query,title)\r\n",
    "    # bigram\r\n",
    "    wc_bigram_q,wc_bigram_t,ratio_bigram_q,ratio_bigram_t = wc_ratio_intersect(getBigram(query),getBigram(title))\r\n",
    "    # trigram\r\n",
    "    wc_trigram_q,wc_trigram_t,ratio_trigram_q,ratio_trigramt = wc_ratio_intersect(getTrigram(query),getTrigram(title))\r\n",
    "    my_list = [wc_unigram_q,wc_unigram_t,ratio_unigram_q,ratio_unigram_t,\r\n",
    "               wc_bigram_q,wc_bigram_t,ratio_bigram_q,ratio_bigram_t,\r\n",
    "               wc_trigram_q,wc_trigram_t,ratio_trigram_q,ratio_trigramt\r\n",
    "              ]\r\n",
    "    return my_list\r\n",
    "\r\n",
    "def trigram_distance_func(query,title):\r\n",
    "    my_list = []\r\n",
    "    # unigram\r\n",
    "    dis_unigram_j = JaccardCoef(query,title)\r\n",
    "    dis_unigram_d = DiceDist(query,title)\r\n",
    "    # bigram\r\n",
    "    dis_bigram_j = JaccardCoef(getBigram(query),getBigram(title))\r\n",
    "    dis_bigram_d = DiceDist(getBigram(query),getBigram(title))\r\n",
    "    # trigram\r\n",
    "    dis_trigram_j = JaccardCoef(getTrigram(query),getTrigram(title))\r\n",
    "    dis_trigram_d = DiceDist(getTrigram(query),getTrigram(title))\r\n",
    "    my_list = [dis_unigram_j,dis_unigram_d,\r\n",
    "               dis_bigram_j,dis_bigram_d,\r\n",
    "               dis_trigram_j,dis_trigram_d\r\n",
    "              ]\r\n",
    "    return my_list\r\n",
    "    \r\n",
    "def trigram_length_func(query,title):\r\n",
    "    my_list = []\r\n",
    "    # unigram\r\n",
    "    dis_unigram = JaccardCoef_Join(query,title)\r\n",
    "    # bigram\r\n",
    "    dis_bigram = JaccardCoef_Join(getBigram(query),getBigram(title))\r\n",
    "    # trigram\r\n",
    "    dis_trigram = JaccardCoef_Join(getTrigram(query),getTrigram(title))\r\n",
    "    my_list = [dis_unigram,\r\n",
    "               dis_bigram,\r\n",
    "               dis_trigram\r\n",
    "              ]\r\n",
    "    return my_list    \r\n",
    "    \r\n",
    "#  相似性度量   最长公共子序列长度 & 最长公共子串的长度\r\n",
    "def lcs_subseq(X, Y): \r\n",
    "    m = len(X) \r\n",
    "    n = len(Y) \r\n",
    "    L = [[None]*(n + 1) for i in range(m + 1)] \r\n",
    "    for i in range(m + 1): \r\n",
    "        for j in range(n + 1): \r\n",
    "            if i == 0 or j == 0 : \r\n",
    "                L[i][j] = 0\r\n",
    "            elif X[i-1] == Y[j-1]: \r\n",
    "                L[i][j] = L[i-1][j-1]+1\r\n",
    "            else: \r\n",
    "                L[i][j] = max(L[i-1][j], L[i][j-1]) \r\n",
    "    return L[m][n] \r\n",
    "\r\n",
    "def lcs_substring(X, Y): \r\n",
    "    m = len(X) \r\n",
    "    n = len(Y) \r\n",
    "    LCSuff = [[0 for k in range(n+1)] for l in range(m+1)] \r\n",
    "    result = 0 \r\n",
    "    # Following steps to build \r\n",
    "    # LCSuff[m+1][n+1] in bottom up fashion \r\n",
    "    for i in range(m + 1): \r\n",
    "        for j in range(n + 1): \r\n",
    "            if (i == 0 or j == 0): \r\n",
    "                LCSuff[i][j] = 0\r\n",
    "            elif (X[i-1] == Y[j-1]): \r\n",
    "                LCSuff[i][j] = LCSuff[i-1][j-1] + 1\r\n",
    "                result = max(result, LCSuff[i][j]) \r\n",
    "            else: \r\n",
    "                LCSuff[i][j] = 0\r\n",
    "    return result\r\n",
    "\r\n",
    "def get_unigram_LCS(X,Y):\r\n",
    "    return [lcs_subseq(X,Y),lcs_substring(X,Y)]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1C55642B16574ABBBEA8C05B3DA450DC",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\ntime: 2.42 ms\n"
     ]
    }
   ],
   "source": [
    "basic_feature_path = feature_path+\"basic_feature.csv\"\r\n",
    "train_data = \"/home/kesci/input/bytedance/first-round/train.csv\"\r\n",
    "samples = 100000000\r\n",
    "chunksize = 5000000\r\n",
    "skip_num = int(samples/chunksize) - 1\r\n",
    "print(skip_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "trusted": true,
    "collapsed": false,
    "id": "79FDB9B6A1DB47678E7611C6DF6D447C",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:31: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "336"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3h 9min 55s\n"
     ]
    }
   ],
   "source": [
    "# # #####################################################\r\n",
    "# # #####  距离特征 & 统计的特征 后500w   用于Train\r\n",
    "# # #####################################################\r\n",
    "\r\n",
    "# basic_feature_fout = open(basic_feature_path,\"w\")\r\n",
    "# df = pd.read_csv(train_data, names=colnames, header=None,skiprows=chunksize*skip_num,nrows=chunksize,lineterminator=\"\\n\")\r\n",
    "# basic_feature = \"\"\r\n",
    "\r\n",
    "# for index, row in df.iterrows():\r\n",
    "#     query_unigram = row[1].split(\" \")\r\n",
    "#     title_unigram = row[3].split(\" \") \r\n",
    "#     # 0 长度特征 (4 +3 dim)\r\n",
    "#     length_q = len(query_unigram)\r\n",
    "#     length_t = len(title_unigram)\r\n",
    "#     length_diff = len(title_unigram)-len(query_unigram)\r\n",
    "#     length_divide = float(len(query_unigram))/len(title_unigram)\r\n",
    "#     length_trigram = trigram_length_func(query_unigram,title_unigram)\r\n",
    "#     # 0 LCS & substring\r\n",
    "#     LCS_feature = get_unigram_LCS(query_unigram,title_unigram)\r\n",
    "#     # 1 距离特征(2*3 dim)\r\n",
    "#     trigram_distance = trigram_distance_func(query_unigram,title_unigram)\r\n",
    "#     # 2 共现单词特征(4*3 dim)\r\n",
    "#     trigram_intersect = trigram_intersect_func(query_unigram,title_unigram)\r\n",
    "\r\n",
    "#     dim_hstack = np.hstack((length_q,length_t,length_diff,length_divide,\r\n",
    "#                             LCS_feature,\r\n",
    "#                             length_trigram,\r\n",
    "#                             trigram_distance,\r\n",
    "#                             trigram_intersect,\r\n",
    "#                           ))\r\n",
    "#     if basic_feature == \"\":\r\n",
    "#         basic_feature = dim_hstack\r\n",
    "#     else:\r\n",
    "#         basic_feature = np.vstack((basic_feature,dim_hstack))\r\n",
    "#     if (index+1) % 100000== 0:\r\n",
    "#         print(\"write_file: \",basic_feature.shape)\r\n",
    "#         np.savetxt(basic_feature_fout,basic_feature,delimiter=\",\",fmt=\"%.5f\")\r\n",
    "#         basic_feature = \"\"\r\n",
    "    \r\n",
    "# basic_feature_fout.flush()\r\n",
    "# del df\r\n",
    "# gc.collect()\r\n",
    "# # 27 dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FC7436C3D4DF4A0F8F16EC7B2B3F9135",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\nquery  (5000000, 5)\ntitle  (5000000, 5)\ntime: 22min 15s\n"
     ]
    }
   ],
   "source": [
    "# # #####################################################\r\n",
    "# # #####  位置特征 后500w   用于Train\r\n",
    "# # #####################################################\r\n",
    "# feature_path = \"/home/kesci/work/counting_feature/train/\"\r\n",
    "# train_data = \"/home/kesci/input/bytedance/first-round/train.csv\"\r\n",
    "# skip_num = int(samples/chunksize) - 1\r\n",
    "# print(skip_num)\r\n",
    "# def get_position_list(target, obs):\r\n",
    "#     \"\"\"\r\n",
    "#         Get the list of positions of obs in target\r\n",
    "#         属于obs & 存在于taget的单词位于obs中的位置\r\n",
    "#         df每一行,返回一个list  (map函数)\r\n",
    "#     \"\"\"\r\n",
    "#     target = target.split(\" \")\r\n",
    "#     obs = obs.split(\" \")\r\n",
    "#     pos_of_obs_in_target = [0]\r\n",
    "#     if len(obs) != 0:\r\n",
    "#         pos_of_obs_in_target = [j for j,w in enumerate(obs, start=1) if w in target]\r\n",
    "#         if len(pos_of_obs_in_target) == 0:\r\n",
    "#             pos_of_obs_in_target = [0]\r\n",
    "#     return pos_of_obs_in_target\r\n",
    "\r\n",
    "# postation_fout = open(feature_path + \"postation.csv\",\"w\")\r\n",
    "# # 1.query\r\n",
    "# df = pd.read_csv(train_data, names=colnames, header=None,skiprows=chunksize*skip_num,nrows=chunksize,lineterminator=\"\\n\")\r\n",
    "# pos = list(df.apply(lambda x: get_position_list(x[\"title\"], obs=x[\"query\"]), axis=1)) #TODO： 1 or ‘columns’: apply function to each row\r\n",
    "# query_hstack = np.hstack((\r\n",
    "#                 np.array(list(map(np.min, pos))).reshape(-1,1),\r\n",
    "#                 np.array(list(map(np.mean, pos))).reshape(-1,1),\r\n",
    "#                 np.array(list(map(np.median, pos))).reshape(-1,1),\r\n",
    "#                 np.array(list(map(np.max, pos))).reshape(-1,1),\r\n",
    "#                 np.array(list(map(np.std, pos))).reshape(-1,1)         \r\n",
    "#     ))\r\n",
    "# print(\"query \",query_hstack.shape)\r\n",
    "# # 2.title\r\n",
    "# pos = list(df.apply(lambda x: get_position_list(x[\"query\"], obs=x[\"title\"]), axis=1)) #TODO： 1 or ‘columns’: apply function to each row\r\n",
    "# title_hstack = np.hstack((\r\n",
    "#                 np.array(list(map(np.min, pos))).reshape(-1,1),\r\n",
    "#                 np.array(list(map(np.mean, pos))).reshape(-1,1),\r\n",
    "#                 np.array(list(map(np.median, pos))).reshape(-1,1),  #重要\r\n",
    "#                 np.array(list(map(np.max, pos))).reshape(-1,1),\r\n",
    "#                 np.array(list(map(np.std, pos))).reshape(-1,1)         \r\n",
    "#     ))\r\n",
    "# print(\"title \",title_hstack.shape)\r\n",
    "# # 3.write to file\r\n",
    "# hstack_all =  np.hstack((query_hstack,title_hstack))\r\n",
    "# np.savetxt(postation_fout,hstack_all,delimiter=\",\",fmt=\"%.5f\")\r\n",
    "# postation_fout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1A548DCCD3BF42C5A360DC4DC784775D"
   },
   "outputs": [],
   "source": [
    "# #####################################################\r\n",
    "# #####  Validation 验证集 \r\n",
    "# #####################################################\r\n",
    "# #####################################################\r\n",
    "# #####  Validation 验证集 \r\n",
    "# #####################################################\r\n",
    "# #####################################################\r\n",
    "# #####  Validation 验证集 \r\n",
    "# #####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "83349FDF47B4411087D9E8AC558E4AFA",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2h 59min 10s\n"
     ]
    }
   ],
   "source": [
    "# # #####################################################\r\n",
    "# # #####  距离特征 & 统计的特征 后1000w-500w   用于validation\r\n",
    "# # #####################################################\r\n",
    "# feature_path = \"/home/kesci/work/counting_feature/valid/\"\r\n",
    "# basic_feature_path = feature_path+\"basic_feature.csv\"\r\n",
    "# train_data = \"/home/kesci/input/bytedance/first-round/train.csv\"\r\n",
    "# samples = 100000000\r\n",
    "# chunksize = 5000000\r\n",
    "# skip_num = int(samples/chunksize) - 2\r\n",
    "# print(skip_num)\r\n",
    "\r\n",
    "# basic_feature_fout = open(basic_feature_path,\"w\")\r\n",
    "# df = pd.read_csv(train_data, names=colnames, header=None,skiprows=chunksize*skip_num,nrows=chunksize,lineterminator=\"\\n\")\r\n",
    "# basic_feature = \"\"\r\n",
    "\r\n",
    "# for index, row in df.iterrows():\r\n",
    "#     query_unigram = row[1].split(\" \")\r\n",
    "#     title_unigram = row[3].split(\" \") \r\n",
    "#     # 0 长度特征 (4 +3 dim)\r\n",
    "#     length_q = len(query_unigram)\r\n",
    "#     length_t = len(title_unigram)\r\n",
    "#     length_diff = len(title_unigram)-len(query_unigram)\r\n",
    "#     length_divide = float(len(query_unigram))/len(title_unigram)\r\n",
    "#     length_trigram = trigram_length_func(query_unigram,title_unigram)\r\n",
    "#     # 0 LCS & substring\r\n",
    "#     LCS_feature = get_unigram_LCS(query_unigram,title_unigram)\r\n",
    "#     # 1 距离特征(2*3 dim)\r\n",
    "#     trigram_distance = trigram_distance_func(query_unigram,title_unigram)\r\n",
    "#     # 2 共现单词特征(4*3 dim)\r\n",
    "#     trigram_intersect = trigram_intersect_func(query_unigram,title_unigram)\r\n",
    "\r\n",
    "#     dim_hstack = np.hstack((length_q,length_t,length_diff,length_divide,\r\n",
    "#                             LCS_feature,\r\n",
    "#                             length_trigram,\r\n",
    "#                             trigram_distance,\r\n",
    "#                             trigram_intersect,\r\n",
    "#                           ))\r\n",
    "#     if basic_feature == \"\":\r\n",
    "#         basic_feature = dim_hstack\r\n",
    "#     else:\r\n",
    "#         basic_feature = np.vstack((basic_feature,dim_hstack))\r\n",
    "#     if (index+1) % 100000== 0:\r\n",
    "#         print(\"write_file: \",basic_feature.shape)\r\n",
    "#         np.savetxt(basic_feature_fout,basic_feature,delimiter=\",\",fmt=\"%.5f\")\r\n",
    "#         basic_feature = \"\"\r\n",
    "    \r\n",
    "# basic_feature_fout.flush()\r\n",
    "# del df\r\n",
    "# gc.collect()\r\n",
    "# # 27 dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "54F6A6E8E6B2441A8BC91B15555CF924",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\nquery  (5000000, 5)\ntitle  (5000000, 5)\ntime: 22min 25s\n"
     ]
    }
   ],
   "source": [
    "# # #####################################################\r\n",
    "# # #####  位置特征 后1000w-500w   用于validation\r\n",
    "# # #####################################################\r\n",
    "# feature_path = \"/home/kesci/work/counting_feature/valid/\"\r\n",
    "# train_data = \"/home/kesci/input/bytedance/first-round/train.csv\"\r\n",
    "# samples = 100000000\r\n",
    "# chunksize = 5000000\r\n",
    "# skip_num = int(samples/chunksize) - 2\r\n",
    "# print(skip_num)\r\n",
    "\r\n",
    "# def get_position_list(target, obs):\r\n",
    "#     \"\"\"\r\n",
    "#         Get the list of positions of obs in target\r\n",
    "#         属于obs & 存在于taget的单词位于obs中的位置\r\n",
    "#         df每一行,返回一个list  (map函数)\r\n",
    "#     \"\"\"\r\n",
    "#     target = target.split(\" \")\r\n",
    "#     obs = obs.split(\" \")\r\n",
    "#     pos_of_obs_in_target = [0]\r\n",
    "#     if len(obs) != 0:\r\n",
    "#         pos_of_obs_in_target = [j for j,w in enumerate(obs, start=1) if w in target]\r\n",
    "#         if len(pos_of_obs_in_target) == 0:\r\n",
    "#             pos_of_obs_in_target = [0]\r\n",
    "#     return pos_of_obs_in_target\r\n",
    "\r\n",
    "# postation_fout = open(feature_path + \"postation.csv\",\"w\")\r\n",
    "# # 1.query\r\n",
    "# df = pd.read_csv(train_data, names=colnames, header=None,skiprows=chunksize*skip_num,nrows=chunksize,lineterminator=\"\\n\")\r\n",
    "# pos = list(df.apply(lambda x: get_position_list(x[\"title\"], obs=x[\"query\"]), axis=1)) #TODO： 1 or ‘columns’: apply function to each row\r\n",
    "# query_hstack = np.hstack((\r\n",
    "#                 np.array(list(map(np.min, pos))).reshape(-1,1),\r\n",
    "#                 np.array(list(map(np.mean, pos))).reshape(-1,1),\r\n",
    "#                 np.array(list(map(np.median, pos))).reshape(-1,1),\r\n",
    "#                 np.array(list(map(np.max, pos))).reshape(-1,1),\r\n",
    "#                 np.array(list(map(np.std, pos))).reshape(-1,1)         \r\n",
    "#     ))\r\n",
    "# print(\"query \",query_hstack.shape)\r\n",
    "# # 2.title\r\n",
    "# pos = list(df.apply(lambda x: get_position_list(x[\"query\"], obs=x[\"title\"]), axis=1)) #TODO： 1 or ‘columns’: apply function to each row\r\n",
    "# title_hstack = np.hstack((\r\n",
    "#                 np.array(list(map(np.min, pos))).reshape(-1,1),\r\n",
    "#                 np.array(list(map(np.mean, pos))).reshape(-1,1),\r\n",
    "#                 np.array(list(map(np.median, pos))).reshape(-1,1),  #重要\r\n",
    "#                 np.array(list(map(np.max, pos))).reshape(-1,1),\r\n",
    "#                 np.array(list(map(np.std, pos))).reshape(-1,1)         \r\n",
    "#     ))\r\n",
    "# print(\"title \",title_hstack.shape)\r\n",
    "# # 3.write to file\r\n",
    "# hstack_all =  np.hstack((query_hstack,title_hstack))\r\n",
    "# np.savetxt(postation_fout,hstack_all,delimiter=\",\",fmt=\"%.5f\")\r\n",
    "# postation_fout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "F188B0D818E34C0A80CADFA0B85DD4C7",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 522 µs\n"
     ]
    }
   ],
   "source": [
    "# #####################################################\r\n",
    "# #####  Test测试集 \r\n",
    "# #####################################################\r\n",
    "# #####################################################\r\n",
    "# #####  Test测试集 \r\n",
    "# #####################################################\r\n",
    "# #####################################################\r\n",
    "# #####  Test测试集 \r\n",
    "# #####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "FBA4624CBD7C4E29A7B41C751252FD98",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:36: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\nwrite_file:  (100000, 27)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "231"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3h 3min 17s\n"
     ]
    }
   ],
   "source": [
    "# # #####################################################\r\n",
    "# # #####  距离特征 & 统计的特征 Test测试集 \r\n",
    "# # #####################################################\r\n",
    "# feature_path = \"/home/kesci/work/counting_feature/test/\"\r\n",
    "# basic_feature_path = feature_path+\"basic_feature.csv\"\r\n",
    "# test_data = \"/home/kesci/input/bytedance/first-round/test.csv\"\r\n",
    "# samples = 100000000\r\n",
    "# chunksize = 5000000\r\n",
    "\r\n",
    "# basic_feature_fout = open(basic_feature_path,\"w\")\r\n",
    "# df = pd.read_csv(test_data, names=colnames, header=None,lineterminator=\"\\n\")\r\n",
    "# basic_feature = \"\"\r\n",
    "\r\n",
    "# for index, row in df.iterrows():\r\n",
    "#     query_unigram = row[1].split(\" \")\r\n",
    "#     title_unigram = row[3].split(\" \") \r\n",
    "#     # 0 长度特征 (4 +3 dim)\r\n",
    "#     length_q = len(query_unigram)\r\n",
    "#     length_t = len(title_unigram)\r\n",
    "#     length_diff = len(title_unigram)-len(query_unigram)\r\n",
    "#     length_divide = float(len(query_unigram))/len(title_unigram)\r\n",
    "#     length_trigram = trigram_length_func(query_unigram,title_unigram)\r\n",
    "#     # 0 LCS & substring\r\n",
    "#     LCS_feature = get_unigram_LCS(query_unigram,title_unigram)\r\n",
    "#     # 1 距离特征(2*3 dim)\r\n",
    "#     trigram_distance = trigram_distance_func(query_unigram,title_unigram)\r\n",
    "#     # 2 共现单词特征(4*3 dim)\r\n",
    "#     trigram_intersect = trigram_intersect_func(query_unigram,title_unigram)\r\n",
    "\r\n",
    "#     dim_hstack = np.hstack((length_q,length_t,length_diff,length_divide,\r\n",
    "#                             LCS_feature,\r\n",
    "#                             length_trigram,\r\n",
    "#                             trigram_distance,\r\n",
    "#                             trigram_intersect,\r\n",
    "#                           ))\r\n",
    "#     if basic_feature == \"\":\r\n",
    "#         basic_feature = dim_hstack\r\n",
    "#     else:\r\n",
    "#         basic_feature = np.vstack((basic_feature,dim_hstack))\r\n",
    "#     if (index+1) % 100000== 0:\r\n",
    "#         print(\"write_file: \",basic_feature.shape)\r\n",
    "#         np.savetxt(basic_feature_fout,basic_feature,delimiter=\",\",fmt=\"%.5f\")\r\n",
    "#         basic_feature = \"\"\r\n",
    "    \r\n",
    "# basic_feature_fout.flush()\r\n",
    "# del df\r\n",
    "# gc.collect()\r\n",
    "# # 27 dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "EF422EF7DCFD421A87ED1E65DE4050CC",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query  (5000000, 5)\ntitle  (5000000, 5)\ntime: 41min 46s\n"
     ]
    }
   ],
   "source": [
    "# # #####################################################\r\n",
    "# # #####  位置特征  Test测试集 \r\n",
    "# # #####################################################\r\n",
    "# feature_path = \"/home/kesci/work/counting_feature/test/\"\r\n",
    "# test_data = \"/home/kesci/input/bytedance/first-round/test.csv\"\r\n",
    "# def get_position_list(target, obs):\r\n",
    "#     \"\"\"\r\n",
    "#         Get the list of positions of obs in target\r\n",
    "#         属于obs & 存在于taget的单词位于obs中的位置\r\n",
    "#         df每一行,返回一个list  (map函数)\r\n",
    "#     \"\"\"\r\n",
    "#     target = target.split(\" \")\r\n",
    "#     obs = obs.split(\" \")\r\n",
    "#     pos_of_obs_in_target = [0]\r\n",
    "#     if len(obs) != 0:\r\n",
    "#         pos_of_obs_in_target = [j for j,w in enumerate(obs, start=1) if w in target]\r\n",
    "#         if len(pos_of_obs_in_target) == 0:\r\n",
    "#             pos_of_obs_in_target = [0]\r\n",
    "#     return pos_of_obs_in_target\r\n",
    "\r\n",
    "# postation_fout = open(feature_path + \"postation.csv\",\"w\")\r\n",
    "# # 1.query\r\n",
    "# df = pd.read_csv(test_data, names=colnames, header=None,lineterminator=\"\\n\")\r\n",
    "# pos = list(df.apply(lambda x: get_position_list(x[\"title\"], obs=x[\"query\"]), axis=1)) #TODO： 1 or ‘columns’: apply function to each row\r\n",
    "# query_hstack = np.hstack((\r\n",
    "#                 np.array(list(map(np.min, pos))).reshape(-1,1),\r\n",
    "#                 np.array(list(map(np.mean, pos))).reshape(-1,1),\r\n",
    "#                 np.array(list(map(np.median, pos))).reshape(-1,1),\r\n",
    "#                 np.array(list(map(np.max, pos))).reshape(-1,1),\r\n",
    "#                 np.array(list(map(np.std, pos))).reshape(-1,1)         \r\n",
    "#     ))\r\n",
    "# print(\"query \",query_hstack.shape)\r\n",
    "# # 2.title\r\n",
    "# pos = list(df.apply(lambda x: get_position_list(x[\"query\"], obs=x[\"title\"]), axis=1)) #TODO： 1 or ‘columns’: apply function to each row\r\n",
    "# title_hstack = np.hstack((\r\n",
    "#                 np.array(list(map(np.min, pos))).reshape(-1,1),\r\n",
    "#                 np.array(list(map(np.mean, pos))).reshape(-1,1),\r\n",
    "#                 np.array(list(map(np.median, pos))).reshape(-1,1),  #重要\r\n",
    "#                 np.array(list(map(np.max, pos))).reshape(-1,1),\r\n",
    "#                 np.array(list(map(np.std, pos))).reshape(-1,1)         \r\n",
    "#     ))\r\n",
    "# print(\"title \",title_hstack.shape)\r\n",
    "# # 3.write to file\r\n",
    "# hstack_all =  np.hstack((query_hstack,title_hstack))\r\n",
    "# np.savetxt(postation_fout,hstack_all,delimiter=\",\",fmt=\"%.5f\")\r\n",
    "# postation_fout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0BDB86217FD84007BBDABD63A3A866A1",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000000\r\n"
     ]
    }
   ],
   "source": [
    "# !cat /home/kesci/work/counting_feature/valid/basic_feature.csv | wc -l\n",
    "# !cat /home/kesci/work/counting_feature/valid/postation.csv | wc -l\n",
    "# !cat /home/kesci/work/counting_feature/train/basic_feature.csv | wc -l\n",
    "# !cat /home/kesci/work/counting_feature/train/postation.csv | wc -l\n",
    "# !cat /home/kesci/work/counting_feature/test/basic_feature.csv | wc -l\n",
    "# !cat /home/kesci/work/counting_feature/test/postation.csv | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "997D05E810E343BF95B07825CA6FF5B1",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "13CDA8BE45C646A1809D5AD3E9AA3F60",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##########################################废弃代码##############################\n",
    "##########################################废弃代码##############################\n",
    "##########################################废弃代码##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "89D85F23B6BC4D6196981F31F218618A",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from numpy import linalg\r\n",
    "from sklearn.model_selection import StratifiedKFold\r\n",
    "import nltk\r\n",
    "import pickle\r\n",
    "from nltk import pos_tag\r\n",
    "from scipy.sparse import hstack, vstack\r\n",
    "from scipy import spatial\r\n",
    "from sklearn.datasets import dump_svmlight_file,load_svmlight_file\r\n",
    "from sklearn.decomposition import TruncatedSVD\r\n",
    "import xgboost as xgb\r\n",
    "import gensim\r\n",
    "from gensim.models import TfidfModel\r\n",
    "from gensim.corpora import Dictionary\r\n",
    "from gensim.models import KeyedVectors\r\n",
    "from sklearn.metrics.pairwise import cosine_similarity\r\n",
    "import multiprocessing\r\n",
    "import gc\r\n",
    "import logging\r\n",
    "# 显示cell运行时长\r\n",
    "%load_ext klab-autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "F6DD2A517158440F93E91983DA225877",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.95 s\n"
     ]
    }
   ],
   "source": [
    "colnames = [\"query_id\",\"query\",\"query_title_id\",\"title\",\"label\"]\n",
    "\n",
    "# # save dict\n",
    "# save_path = \"/home/kesci/work/word2vec/wordvectors.kv\"\n",
    "# w2vmodel_test = pickle.load(open(model_path+\"word2vec_model_new_withTest.pkl\",\"rb\"))\n",
    "# w2vmodel_test.wv.save(save_path)\n",
    "save_path = \"/home/kesci/work/word2vec/wordvectors.kv\"\n",
    "w2v_dict = KeyedVectors.load(save_path, mmap='r')\n",
    "len(w2v_dict[\"1427\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35EEC129C8EF4E53A46FE876D4710865"
   },
   "outputs": [],
   "source": [
    "############################## avg word2vec 表征 sentence vector ##############\n",
    "############################## avg word2vec 表征 sentence vector ##############\n",
    "############################## avg word2vec 表征 sentence vector ##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "80BEB50F0C624D8180793EF4E85F5A8B",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.09 ms\n"
     ]
    }
   ],
   "source": [
    "def cosine_sim(query, title):\r\n",
    "    l2_query = linalg.norm(query,ord=2,axis=1)\r\n",
    "    l2_title = linalg.norm(title,ord=2,axis=1)\r\n",
    "    eudistance = linalg.norm(query-title,ord=2,axis=1) # 欧式距离 L2\r\n",
    "    manhattan = linalg.norm(query-title,ord=1,axis=1) # 曼哈顿距离 L1\r\n",
    "    chebyshev = linalg.norm(query-title,ord=np.inf,axis=1) # 切比雪夫距离 Max|x1-x2|\r\n",
    "    chebyshev_ = linalg.norm(query-title,ord=-np.inf,axis=1) # Min |x1-x2|\r\n",
    "    my_sqrt_distance = np.sum(np.sqrt(np.absolute(query-title)),axis=1) # sqrt|x1-x2|\r\n",
    "    sim = np.sum(query * title,axis=1)/(l2_query * l2_title)\r\n",
    "    \r\n",
    "    combine_sim = np.hstack((sim.reshape(-1,1),\r\n",
    "                             eudistance.reshape(-1,1),\r\n",
    "                             manhattan.reshape(-1,1),\r\n",
    "                             chebyshev.reshape(-1,1),chebyshev_.reshape(-1,1),\r\n",
    "                             my_sqrt_distance.reshape(-1,1)\r\n",
    "                            ))\r\n",
    "    return combine_sim\r\n",
    "\r\n",
    "#### 1w大小的数据  计算一次\r\n",
    "def concat_sentence(origin_data):\r\n",
    "    q_vstack = \"\"\r\n",
    "    t_vstack = \"\"\r\n",
    "    for row in origin_data.values:\r\n",
    "        # 1.query\r\n",
    "        words = row[1].split(\" \")\r\n",
    "        M = []\r\n",
    "        for w in words:\r\n",
    "            try:\r\n",
    "                M.append(w2v_dict[w])\r\n",
    "            except:\r\n",
    "                continue\r\n",
    "        M = np.array(M)\r\n",
    "        if M.shape[0] != 0:\r\n",
    "            q_sentence_vec = M.sum(axis=0) / M.shape[0]\r\n",
    "        else:\r\n",
    "            q_sentence_vec = np.full(word_dim,np.finfo(np.float32).eps)\r\n",
    "        if q_vstack == \"\":\r\n",
    "            q_vstack = q_sentence_vec\r\n",
    "        else:\r\n",
    "             q_vstack = np.vstack((q_vstack,q_sentence_vec))\r\n",
    "        \r\n",
    "        # 2.title\r\n",
    "        words = row[3].split(\" \")\r\n",
    "        M = []\r\n",
    "        for w in words:\r\n",
    "            try:\r\n",
    "                M.append(w2v_dict[w])\r\n",
    "            except:\r\n",
    "                continue\r\n",
    "        M = np.array(M)\r\n",
    "        if M.shape[0] != 0:\r\n",
    "            t_sentence_vec = M.sum(axis=0) / M.shape[0]\r\n",
    "        else:\r\n",
    "            t_sentence_vec = np.full(word_dim,np.finfo(np.float32).eps)\r\n",
    "        if t_vstack == \"\":\r\n",
    "            t_vstack = t_sentence_vec\r\n",
    "        else:\r\n",
    "             t_vstack = np.vstack((t_vstack,t_sentence_vec))\r\n",
    "    return q_vstack,t_vstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "158585E67A2F42288A94AB5DD4A2DFCA",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:37: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:55: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 128) (10000, 6)\n(10000, 128) (10000, 6)\n(10000, 128) (10000, 6)\n(10000, 128) (10000, 6)\n(10000, 128) (10000, 6)\ntime: 1h 9min 59s\n"
     ]
    }
   ],
   "source": [
    "############## 测试集数据 ############\r\n",
    "feature_path = \"/home/kesci/work/similarity_feature/test/\"\r\n",
    "test_data = \"/home/kesci/input/bytedance/first-round/test.csv\"\r\n",
    "# sentece vector of (query-title)\r\n",
    "sentence_vector_path = feature_path+\"sentence_vector.csv\"\r\n",
    "sentence_vec_fout = open(sentence_vector_path,\"w\")\r\n",
    "# 6dim similarity feature\r\n",
    "consine_feature_path = feature_path+\"consine_sentence_sim.csv\"\r\n",
    "sim_qt_fout = open(consine_feature_path,\"w\")\r\n",
    "\r\n",
    "chunksize = 10000\r\n",
    "\r\n",
    "for i in range(int(500)):\r\n",
    "    df = pd.read_csv(test_data, names=colnames, header=None,skiprows=chunksize*i,nrows=chunksize,lineterminator=\"\\n\")\r\n",
    "    # 1.get sentece_vector & consine sim\r\n",
    "    q_vstack,t_vstack = concat_sentence(df)\r\n",
    "    combine_sim = cosine_sim(q_vstack,t_vstack)\r\n",
    "    # 2.save\r\n",
    "    np.savetxt(sentence_vec_fout,q_vstack - t_vstack,delimiter=\",\",fmt=\"%.5f\")\r\n",
    "    np.savetxt(sim_qt_fout,combine_sim,delimiter=\",\",fmt=\"%.5f\")\r\n",
    "\r\n",
    "    # 3.print log\r\n",
    "    if (i+1) % 100 == 0:\r\n",
    "        print((q_vstack - t_vstack).shape,combine_sim.shape)\r\n",
    "    # # 4.gc\r\n",
    "    # del df,q_vstack,t_vstack\r\n",
    "    # gc.collect()\r\n",
    "\r\n",
    "sentence_vec_fout.flush()\r\n",
    "sim_qt_fout.flush()\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8761C35BD90841518CCBF59B92F86589",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000000\n5000000\ntime: 6.85 s\n"
     ]
    }
   ],
   "source": [
    "!cat /home/kesci/work/similarity_feature/test/consine_sentence_sim.csv | wc -l\n",
    "!cat /home/kesci/work/similarity_feature/test/sentence_vector.csv | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "3339D66FC5DF4E098D027E11B5B567E0",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.4 ms\n"
     ]
    }
   ],
   "source": [
    "##################################################\r\n",
    "##########计算word 和title_sentence sim\r\n",
    "##################################################\r\n",
    "word_dim = 128\r\n",
    "from scipy import spatial\r\n",
    "def cosine_sim(A, B):\r\n",
    "    return 1 - spatial.distance.cosine(A, B)\r\n",
    "\r\n",
    "def word_sentence_sim(origin_data):\r\n",
    "    sim_vstack = \"\"\r\n",
    "    for row in origin_data.values:        \r\n",
    "        # 1. title sentence vector\r\n",
    "        words = nltk.SpaceTokenizer().tokenize(row[3])\r\n",
    "        t_sentence_vec = np.full(word_dim,np.finfo(np.float32).eps)\r\n",
    "        M = []\r\n",
    "        for w in words:\r\n",
    "            try:\r\n",
    "                M.append(w2v_dict[w])\r\n",
    "            except:\r\n",
    "                continue\r\n",
    "        M = np.array(M)\r\n",
    "        if M.shape[0] != 0:\r\n",
    "            t_sentence_vec = M.sum(axis=0) / M.shape[0]\r\n",
    "        \r\n",
    "        # 2.get max,min,avg,median,std\r\n",
    "        words = nltk.SpaceTokenizer().tokenize(row[1])\r\n",
    "        M = []\r\n",
    "        for w in words:\r\n",
    "            try:\r\n",
    "                A = w2v_dict[w]\r\n",
    "                M.append(cosine_sim(A,t_sentence_vec))\r\n",
    "            except:\r\n",
    "                continue\r\n",
    "        M = np.array(M)\r\n",
    "        if M.shape[0] == 0:\r\n",
    "            M = np.zeros(word_dim)\r\n",
    "        sim_hstack = np.hstack(( \r\n",
    "                            np.min(M),np.max(M),\r\n",
    "                            np.average(M),np.median(M),np.std(M)\r\n",
    "                               ))         \r\n",
    "        if sim_vstack == \"\":\r\n",
    "            sim_vstack = sim_hstack.reshape(1,-1)\r\n",
    "        else:\r\n",
    "            sim_vstack = np.vstack((sim_vstack,sim_hstack.reshape(1,-1)))\r\n",
    "    return sim_vstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "8F13081E8854495E855B469B77CCD289",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:41: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 5)\n(10000, 5)\n(10000, 5)\n(10000, 5)\n(10000, 5)\ntime: 1h 8min 9s\n"
     ]
    }
   ],
   "source": [
    "############## 1000w-500w 验证集数据 ############\r\n",
    "feature_path = \"/home/kesci/work/similarity_feature/valid/\"\r\n",
    "train_data = \"/home/kesci/input/bytedance/first-round/train.csv\"\r\n",
    "# # 5dim word_similarity feature\r\n",
    "consine_feature_path = feature_path+\"consine_word_sim.csv\"\r\n",
    "sim_qt_fout = open(consine_feature_path,\"w\")\r\n",
    "\r\n",
    "samples = 100000000\r\n",
    "chunksize = 10000\r\n",
    "skip_num = int(samples/chunksize) - 1000\r\n",
    "print(skip_num)\r\n",
    "\r\n",
    "for i in range(int(500)):\r\n",
    "    df = pd.read_csv(train_data, names=colnames, header=None,skiprows=chunksize*i,nrows=chunksize,lineterminator=\"\\n\")\r\n",
    "    combine_sim = word_sentence_sim(df)\r\n",
    "    np.savetxt(sim_qt_fout,combine_sim,delimiter=\",\",fmt=\"%.5f\") \r\n",
    "    # print log\r\n",
    "    if (i+1) % 100 == 0:\r\n",
    "        print(combine_sim.shape)\r\n",
    "\r\n",
    "sim_qt_fout.flush()\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "F211DD74B8DF42718C83732C68234745",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:41: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 5)\n(10000, 5)\n(10000, 5)\n(10000, 5)\n(10000, 5)\ntime: 1h 6min 28s\n"
     ]
    }
   ],
   "source": [
    "############## 测试集数据 ############\r\n",
    "feature_path = \"/home/kesci/work/similarity_feature/test/\"\r\n",
    "test_data = \"/home/kesci/input/bytedance/first-round/test.csv\"\r\n",
    "# 5dim word_similarity feature\r\n",
    "consine_feature_path = feature_path+\"consine_word_sim.csv\"\r\n",
    "sim_qt_fout = open(consine_feature_path,\"w\")\r\n",
    "\r\n",
    "chunksize = 10000\r\n",
    "\r\n",
    "for i in range(int(500)):\r\n",
    "    df = pd.read_csv(test_data, names=colnames, header=None,skiprows=chunksize*i,nrows=chunksize,lineterminator=\"\\n\")\r\n",
    "    combine_sim = word_sentence_sim(df)\r\n",
    "    np.savetxt(sim_qt_fout,combine_sim,delimiter=\",\",fmt=\"%.5f\") \r\n",
    "    # print log\r\n",
    "    if (i+1) % 100 == 0:\r\n",
    "        print(combine_sim.shape)\r\n",
    "\r\n",
    "sim_qt_fout.flush()\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24ACCB9D24EA497B8448A1342781E66A"
   },
   "outputs": [],
   "source": [
    "##############################  点击量特征 2 dim ####################\n",
    "##############################  点击量特征 2 dim ####################\n",
    "##############################  点击量特征 2 dim ####################\n",
    "##############################  点击量特征 2 dim ####################\n",
    "##############################  点击量特征 2 dim ####################\n",
    "##############################  点击量特征 2 dim ####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "A7A0A17FD4EC4E038E20243011F0D657",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4min 5s\n"
     ]
    }
   ],
   "source": [
    "#### 训练集分别对于query_id,title保存点击量特征\n",
    "feature_path = \"/home/kesci/work/counting_feature/train/\"\n",
    "train_data = \"/home/kesci/input/bytedance/first-round/train.csv\"\n",
    "chunksize = 10000\n",
    "queryID_dict = dict()\n",
    "for df in pd.read_csv(train_data, names=colnames, header=None,chunksize=chunksize,lineterminator=\"\\n\"):\n",
    "    for row in df.values:\n",
    "        qid = row[0]\n",
    "        queryID_dict[qid] = queryID_dict.get(qid,0) + 1\n",
    "\n",
    "# # 训练集title点击量特征\n",
    "# title_dict = dict()\n",
    "# for df in pd.read_csv(train_data, names=colnames, header=None,chunksize=chunksize,lineterminator=\"\\n\"):\n",
    "#     for row in df.values:\n",
    "#         title = row[3]\n",
    "#         title_dict[title] = title_dict.get(title,0) + 1\n",
    "\n",
    "\n",
    "# 训练集title:query_list(5dim) 特征   \n",
    "print(len(queryID_dict.keys()))\n",
    "title_dict = dict()\n",
    "for df in pd.read_csv(train_data, names=colnames, header=None,chunksize=chunksize,lineterminator=\"\\n\"):\n",
    "    for row in df.values:\n",
    "        qid = row[0]\n",
    "        title = row[3]\n",
    "        title_dict[title] = title_dict.get(title,\"\")+ str(queryID_dict[qid]) + \",\"\n",
    "del queryID_dict\n",
    "gc.collect()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "F221808957454DE6B9C7EDD71094539F",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28934366\ntime: 572 µs\n"
     ]
    }
   ],
   "source": [
    "# print(len(queryID_dict.keys()))\n",
    "print(len(title_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "796A5BAAC5594C2F85F0DC0A9EDB30A2",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n(1, 5)\n(1, 5)\n(1, 5)\n(1, 5)\n(1, 5)\ntime: 36min 50s\n"
     ]
    }
   ],
   "source": [
    "# ############################################# 06.17   5dim     title:query_list \n",
    "# ############################################# 06.17   5dim     title:query_list \n",
    "# ############################################# 06.17   5dim     title:query_list \n",
    "# ############################################# 06.17   5dim     title:query_list \n",
    "# ############################################# 06.17   5dim     title:query_list \n",
    "# def get_5dim_intersect(q_list):\n",
    "#     query_np = np.array(q_list)\n",
    "#     return [np.min(query_np),\n",
    "#             np.max(query_np),\n",
    "#             np.median(query_np),\n",
    "#             np.mean(query_np),\n",
    "#             np.std(query_np),\n",
    "#             ]\n",
    "# samples = 100000000\n",
    "# chunksize = 5000000\n",
    "# # 训练集 (500w)\n",
    "# feature_path = \"/home/kesci/work/counting_feature/train/\"\n",
    "# train_data = \"/home/kesci/input/bytedance/first-round/train.csv\"\n",
    "# skip_num = int(samples/chunksize) - 1\n",
    "# print(skip_num)\n",
    "# title_path = feature_path + \"click_num_t_groupby_5dim.csv\"\n",
    "# title_feature_fout = open(title_path,\"w\")\n",
    "# df = pd.read_csv(train_data, names=colnames, header=None,skiprows=chunksize*skip_num,nrows=chunksize,lineterminator=\"\\n\")\n",
    "# for index, row in df.iterrows():\n",
    "#     qclick_list_str = title_dict[row[3]].split(\",\")[:-1]\n",
    "#     qclick_list_int = [ int(x) for x in qclick_list_str ]\n",
    "#     query_list_5dim = np.array(get_5dim_intersect(qclick_list_int)).reshape(1,-1)\n",
    "#     np.savetxt(title_feature_fout,query_list_5dim,delimiter=\",\",fmt=\"%.5f\")\n",
    "#     if index % 1000000 == 0:\n",
    "#         print(query_list_5dim.shape)\n",
    "# title_feature_fout.flush()\n",
    "\n",
    "# # 验证集 (1000w - 500w)\n",
    "# feature_path = \"/home/kesci/work/counting_feature/valid/\"\n",
    "# train_data = \"/home/kesci/input/bytedance/first-round/train.csv\"\n",
    "# skip_num = int(samples/chunksize) - 2\n",
    "# print(skip_num)\n",
    "# title_path = feature_path + \"click_num_t_groupby_5dim.csv\"\n",
    "# title_feature_fout = open(title_path,\"w\")\n",
    "# df = pd.read_csv(train_data, names=colnames, header=None,skiprows=chunksize*skip_num,nrows=chunksize,lineterminator=\"\\n\")\n",
    "# for index, row in df.iterrows():\n",
    "#     qclick_list_str = title_dict[row[3]].split(\",\")[:-1]\n",
    "#     qclick_list_int = [ int(x) for x in qclick_list_str ]\n",
    "#     query_list_5dim = np.array(get_5dim_intersect(qclick_list_int)).reshape(1,-1)\n",
    "#     np.savetxt(title_feature_fout,query_list_5dim,delimiter=\",\",fmt=\"%.5f\")\n",
    "#     if index % 1000000 == 0:\n",
    "#         print(query_list_5dim.shape)\n",
    "# title_feature_fout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "F9E879FEBB83414882EDF765C55471E8",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n0\n1000000\n2000000\n3000000\n4000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16min 1s\n"
     ]
    }
   ],
   "source": [
    "###################################################### 2dim to file\n",
    "###################################################### 2dim to file\n",
    "###################################################### 2dim to file\n",
    "# # merge train\n",
    "# skip_num = int(samples/chunksize) - 1\n",
    "# print(skip_num)\n",
    "# merge_q_fout = open(feature_path+\"click_num_q.csv\",\"w\")\n",
    "# merge_t_fout = open(feature_path+\"click_num_t.csv\",\"w\")\n",
    "# df = pd.read_csv(train_data, names=colnames, header=None,skiprows=chunksize*skip_num,nrows=chunksize,lineterminator=\"\\n\")\n",
    "# # 3:4,1:2.3\n",
    "# for index, row in df.iterrows():\n",
    "#     q_count = queryID_dict[row[0]]\n",
    "#     t_count = float(title_dict[row[3]])\n",
    "#     merge_q_fout.write(str(q_count)+\"\\n\")\n",
    "#     merge_t_fout.write(str(t_count)+\"\\n\")\n",
    "#     if(index % 1000000 == 0):\n",
    "#         print(index)\n",
    "# merge_q_fout.flush()    \n",
    "# merge_t_fout.flush()\n",
    "# del df\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "959CF6EC62AC429381033D25538FDF41",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n0\n1000000\n2000000\n3000000\n4000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16min 15s\n"
     ]
    }
   ],
   "source": [
    "# # merge valid\n",
    "# feature_path = \"/home/kesci/work/counting_feature/valid/\"\n",
    "# skip_num = int(samples/chunksize) - 2\n",
    "# print(skip_num)\n",
    "# merge_q_fout = open(feature_path+\"click_num_q.csv\",\"w\")\n",
    "# merge_t_fout = open(feature_path+\"click_num_t.csv\",\"w\")\n",
    "# df = pd.read_csv(train_data, names=colnames, header=None,skiprows=chunksize*skip_num,nrows=chunksize,lineterminator=\"\\n\")\n",
    "# # 3:4,1:2.3\n",
    "# for index, row in df.iterrows():\n",
    "#     q_count = queryID_dict[row[0]]\n",
    "#     t_count = float(title_dict[row[3]])\n",
    "#     merge_q_fout.write(str(q_count)+\"\\n\")\n",
    "#     merge_t_fout.write(str(t_count)+\"\\n\")\n",
    "#     if(index % 1000000 == 0):\n",
    "#         print(index)\n",
    "# merge_q_fout.flush()    \n",
    "# merge_t_fout.flush()\n",
    "# del df\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "E5C3C79ED5C24A1DAB419B89A4624CCB",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 33.1 s\n"
     ]
    }
   ],
   "source": [
    "# ####################################################点击量特征 Test 集\n",
    "# ####################################################点击量特征 Test 集\n",
    "# ####################################################点击量特征 Test 集\n",
    "# ####################################################点击量特征 Test 集\n",
    "# ####################################################点击量特征 Test 集\n",
    "### 测试集分别对于query_id,title保存点击量特征\n",
    "query_dict_test = dict()\n",
    "test_data = \"/home/kesci/input/bytedance/first-round/test.csv\"\n",
    "for df in pd.read_csv(test_data, names=colnames, header=None,chunksize=chunksize,lineterminator=\"\\n\"):\n",
    "    for row in df.values:\n",
    "        title = row[0]\n",
    "        query_dict_test[title] = query_dict_test.get(title,0) + 1\n",
    "\n",
    "# # title_dict_test = dict()\n",
    "# # test_data = \"/home/kesci/input/bytedance/first-round/test.csv\"\n",
    "# # for df in pd.read_csv(test_data, names=colnames, header=None,chunksize=chunksize,lineterminator=\"\\n\"):\n",
    "# #     for row in df.values:\n",
    "# #         title = row[3]\n",
    "# #         title_dict_test[title] = title_dict_test.get(title,0) + 1\n",
    "        \n",
    "# 测试集 title:query_list(5dim) 特征   \n",
    "title_dict_test = dict()\n",
    "for df in pd.read_csv(test_data, names=colnames, header=None,chunksize=chunksize,lineterminator=\"\\n\"):\n",
    "    for row in df.values:\n",
    "        qid = row[0]\n",
    "        title = row[3]\n",
    "        title_dict_test[title] = title_dict_test.get(title,\"\")+ str(query_dict_test[qid]) + \",\"\n",
    "del query_dict_test\n",
    "gc.collect()\n",
    "\n",
    "print(len(title_dict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4C4087499132474E9F1C8928AB7296CE",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5)\n(1, 5)\n(1, 5)\n(1, 5)\n(1, 5)\ntime: 35min 44s\n"
     ]
    }
   ],
   "source": [
    "############################################# 06.17   5dim     title:query_list \n",
    "############################################# 06.17   5dim     title:query_list \n",
    "# 测试集\n",
    "feature_path = \"/home/kesci/work/counting_feature/test/\"\n",
    "test_data = \"/home/kesci/input/bytedance/first-round/test.csv\"\n",
    "title_path = feature_path + \"click_num_t_groupby_5dim.csv\"\n",
    "title_feature_fout = open(title_path,\"w\")\n",
    "df = pd.read_csv(test_data, names=colnames, header=None,lineterminator=\"\\n\")\n",
    "for index, row in df.iterrows():\n",
    "    qclick_list_str = title_dict_test[row[3]].split(\",\")[:-1]\n",
    "    qclick_list_int = [ int(x) for x in qclick_list_str ]\n",
    "    query_list_5dim = np.array(get_5dim_intersect(qclick_list_int)).reshape(1,-1)\n",
    "    np.savetxt(title_feature_fout,query_list_5dim,delimiter=\",\",fmt=\"%.5f\")\n",
    "    if index % 1000000 == 0:\n",
    "        print(query_list_5dim.shape)\n",
    "title_feature_fout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95B3300DB15242B488CD0C77CF61FB0F"
   },
   "outputs": [],
   "source": [
    "####################################################################### 2dim 测试集\n",
    "####################################################################### 2dim 测试集\n",
    "####################################################################### 2dim 测试集\n",
    "# feature_path = \"/home/kesci/work/counting_feature/test/\"\n",
    "# test_data = \"/home/kesci/input/bytedance/first-round/test.csv\"\n",
    "# chunksize = 10000\n",
    "# title_dict_test = dict()\n",
    "# for df in pd.read_csv(test_data, names=colnames, header=None,chunksize=chunksize,lineterminator=\"\\n\"):\n",
    "#     for row in df.values:\n",
    "#         title = row[3]\n",
    "#         title_dict_test[title] = title_dict_test.get(title,0) + 1\n",
    "# query_dict_test = dict()\n",
    "# for df in pd.read_csv(test_data, names=colnames, header=None,chunksize=chunksize,lineterminator=\"\\n\"):\n",
    "#     for row in df.values:\n",
    "#         title = row[0]\n",
    "#         query_dict_test[title] = query_dict_test.get(title,0) + 1\n",
    "\n",
    "# # merge test\n",
    "# merge_q_fout = open(feature_path+\"click_num_q.csv\",\"w\")\n",
    "# merge_t_fout = open(feature_path+\"click_num_t.csv\",\"w\")\n",
    "# df = pd.read_csv(test_data, names=colnames, header=None,lineterminator=\"\\n\")\n",
    "# # 3:4,1:2.3  (test title 不作处理)\n",
    "# for index, row in df.iterrows():\n",
    "#     q_count = query_dict_test[row[0]]\n",
    "#     t_count = title_dict_test[row[3]]\n",
    "#     merge_q_fout.write(str(q_count)+\"\\n\")\n",
    "#     merge_t_fout.write(str(t_count)+\"\\n\")\n",
    "#     if (index % 1000000 == 0):\n",
    "#         print(index)\n",
    "# merge_q_fout.flush()    \n",
    "# merge_t_fout.flush()\n",
    "# del df\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9824DA6F8C0E4E9C87A3149F9FA2E47A"
   },
   "outputs": [],
   "source": [
    "##############################  点击量特征 2+5+5 dim (06.17新版本)####################\n",
    "##############################  点击量特征 2+5+5 dim (06.17新版本)####################\n",
    "##############################  点击量特征 2+5+5 dim (06.17新版本)####################\n",
    "##############################  点击量特征 2+5+5 dim (06.17新版本)####################\n",
    "##############################  点击量特征 2+5+5 dim (06.17新版本)####################\n",
    "##############################  点击量特征 2+5+5 dim (06.17新版本)####################\n",
    "##############################  点击量特征 2+5+5 dim (06.17新版本)####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7D4D2FFD10B043FBA0AA8A174E888C21",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n(5000000,) (5000000, 1) (5000000, 1) (5000000, 5)\n(5000000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>query_id</th>\n      <th>title_num</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>14315375</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>14315375</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>14315375</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>14315375</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>14315376</td>\n      <td>6.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
      ],
      "text/plain": [
       "   query_id  title_num\n0  14315375        3.0\n1  14315375        9.0\n2  14315375        1.0\n3  14315375        4.0\n4  14315376        6.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 38.4 s\n"
     ]
    }
   ],
   "source": [
    "#################################### train ##########################\n",
    "#################################### train ##########################\n",
    "#################################### train ##########################\n",
    "feature_path = \"/home/kesci/work/counting_feature/train/\"\n",
    "train_data = \"/home/kesci/input/bytedance/first-round/train.csv\"\n",
    "#已经计算完毕的 query,title的点击量特征\n",
    "click_query_path = feature_path + \"click_num_q.csv\" \n",
    "click_title_path = feature_path + \"click_num_t.csv\"\n",
    "click_title_5dim_path = feature_path + \"click_num_t_groupby_5dim.csv\"\n",
    "df_query = pd.read_csv(click_query_path, header=None,lineterminator=\"\\n\")\n",
    "df_title = pd.read_csv(click_title_path, names=[\"title_num\"], header=None,lineterminator=\"\\n\")\n",
    "df_title_groupby = pd.read_csv(click_title_5dim_path, header=None,lineterminator=\"\\n\")\n",
    "# combine 点击量特征\n",
    "click_combine_path = feature_path + \"click_num_12dim.csv\"\n",
    "\n",
    "samples = 100000000\n",
    "chunksize = 5000000\n",
    "skip_num = int(samples/chunksize) - 1\n",
    "print(skip_num)\n",
    "\n",
    "df_qid = pd.read_csv(train_data, names=colnames, header=None,\n",
    "                skiprows=chunksize*skip_num,nrows=chunksize,lineterminator=\"\\n\")[\"query_id\"]\n",
    "df = pd.concat([df_qid, df_title], axis=1)\n",
    "# 已保存的文件\n",
    "print(df_qid.shape,df_query.shape,df_title.shape,df_title_groupby.shape)\n",
    "# concat 新文件\n",
    "print(df.shape)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9869E56C112A41698EA11E4F2C84E37F",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:9: FutureWarning: using a dict on a Series for aggregation\nis deprecated and will be removed in a future version\n  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000000, 12)\ntime: 10min 50s\n"
     ]
    }
   ],
   "source": [
    "click_combine_fout = open(click_combine_path,\"w\")\r\n",
    "aggregation={\r\n",
    "        \"MIN\": lambda x: x.min(skipna=True),\r\n",
    "        \"MAX\":lambda x: x.max(skipna=True),\r\n",
    "        \"MEDIAN\":lambda x: x.median(skipna=True),\r\n",
    "        \"MEAN\":lambda x:x.mean(skipna=True),\r\n",
    "        \"STD\":lambda x:x.std(skipna=True),\r\n",
    "}\r\n",
    "df_tmp = df.groupby(\"query_id\", sort=False)[\"title_num\"].agg(aggregation).reset_index()\r\n",
    "new_df = df.merge(df_tmp,on=['query_id'])[['MIN', 'MAX', 'MEDIAN', 'MEAN', 'STD']]\r\n",
    "hstack_all = np.hstack((\r\n",
    "                    np.array(df_query).reshape(-1,1), # query 点击量\r\n",
    "                    np.array(df_title).reshape(-1,1), # title 点击量\r\n",
    "                    np.array(df_title_groupby),\r\n",
    "                    np.array(new_df),\r\n",
    "                    ))\r\n",
    "print(hstack_all.shape)\r\n",
    "np.savetxt(click_combine_fout,hstack_all,delimiter=\",\",fmt=\"%.5f\")\r\n",
    "click_combine_fout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "3C26B4960D124B75BE3623F5D2F6B170",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n(5000000,) (5000000, 1) (5000000, 1) (5000000, 5)\n(5000000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>query_id</th>\n      <th>title_num</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>13556849</td>\n      <td>45.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>13556849</td>\n      <td>8.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13556849</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>13556849</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>13556849</td>\n      <td>27.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
      ],
      "text/plain": [
       "   query_id  title_num\n0  13556849       45.0\n1  13556849        8.0\n2  13556849        2.0\n3  13556849        5.0\n4  13556849       27.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 37.3 s\n"
     ]
    }
   ],
   "source": [
    "#################################### valid ##########################\n",
    "#################################### valid ##########################\n",
    "#################################### valid ##########################\n",
    "feature_path = \"/home/kesci/work/counting_feature/valid/\"\n",
    "train_data = \"/home/kesci/input/bytedance/first-round/train.csv\"\n",
    "#已经计算完毕的 query,title的点击量特征\n",
    "click_query_path = feature_path + \"click_num_q.csv\" \n",
    "click_title_path = feature_path + \"click_num_t.csv\"\n",
    "click_title_5dim_path = feature_path + \"click_num_t_groupby_5dim.csv\"\n",
    "df_query = pd.read_csv(click_query_path, header=None,lineterminator=\"\\n\")\n",
    "df_title = pd.read_csv(click_title_path, names=[\"title_num\"], header=None,lineterminator=\"\\n\")\n",
    "df_title_groupby = pd.read_csv(click_title_5dim_path, header=None,lineterminator=\"\\n\")\n",
    "# combine 点击量特征\n",
    "click_combine_path = feature_path + \"click_num_12dim.csv\"\n",
    "\n",
    "samples = 100000000\n",
    "chunksize = 5000000\n",
    "skip_num = int(samples/chunksize) - 2\n",
    "print(skip_num)\n",
    "\n",
    "df_qid = pd.read_csv(train_data, names=colnames, header=None,\n",
    "                skiprows=chunksize*skip_num,nrows=chunksize,lineterminator=\"\\n\")[\"query_id\"]\n",
    "df = pd.concat([df_qid, df_title], axis=1)\n",
    "# 已保存的文件\n",
    "print(df_qid.shape,df_query.shape,df_title.shape,df_title_groupby.shape)\n",
    "# concat 新文件\n",
    "print(df.shape)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "7F15F0CA23A4480B8E7025EFBF2FBAA5",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:9: FutureWarning: using a dict on a Series for aggregation\nis deprecated and will be removed in a future version\n  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000000, 12)\ntime: 10min 50s\n"
     ]
    }
   ],
   "source": [
    "click_combine_fout = open(click_combine_path,\"w\")\r\n",
    "aggregation={\r\n",
    "        \"MIN\": lambda x: x.min(skipna=True),\r\n",
    "        \"MAX\":lambda x: x.max(skipna=True),\r\n",
    "        \"MEDIAN\":lambda x: x.median(skipna=True),\r\n",
    "        \"MEAN\":lambda x:x.mean(skipna=True),\r\n",
    "        \"STD\":lambda x:x.std(skipna=True),\r\n",
    "}\r\n",
    "df_tmp = df.groupby(\"query_id\", sort=False)[\"title_num\"].agg(aggregation).reset_index()\r\n",
    "new_df = df.merge(df_tmp,on=['query_id'])[['MIN', 'MAX', 'MEDIAN', 'MEAN', 'STD']]\r\n",
    "hstack_all = np.hstack((\r\n",
    "                    np.array(df_query).reshape(-1,1),\r\n",
    "                    np.array(df_title).reshape(-1,1),\r\n",
    "                    np.array(df_title_groupby),\r\n",
    "                    np.array(new_df),\r\n",
    "                    ))\r\n",
    "print(hstack_all.shape)\r\n",
    "np.savetxt(click_combine_fout,hstack_all,delimiter=\",\",fmt=\"%.5f\")\r\n",
    "click_combine_fout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "253292940C07444C8C44CE3D559E77F2",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000000,) (5000000, 1) (5000000, 1) (5000000, 5)\n(5000000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>query_id</th>\n      <th>title_num</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
      ],
      "text/plain": [
       "   query_id  title_num\n0         1          1\n1         1          1\n2         1          1\n3         1          1\n4         2          4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 13.4 s\n"
     ]
    }
   ],
   "source": [
    "#################################### test ##########################\n",
    "#################################### test ##########################\n",
    "#################################### test ##########################\n",
    "feature_path = \"/home/kesci/work/counting_feature/test/\"\n",
    "test_data = \"/home/kesci/input/bytedance/first-round/test.csv\"\n",
    "#已经计算完毕的 query,title的点击量特征\n",
    "click_query_path = feature_path + \"click_num_q.csv\" \n",
    "click_title_path = feature_path + \"click_num_t.csv\"\n",
    "click_title_5dim_path = feature_path + \"click_num_t_groupby_5dim.csv\"\n",
    "df_query = pd.read_csv(click_query_path, header=None,lineterminator=\"\\n\")\n",
    "df_title = pd.read_csv(click_title_path, names=[\"title_num\"], header=None,lineterminator=\"\\n\")\n",
    "df_title_groupby = pd.read_csv(click_title_5dim_path,header=None,lineterminator=\"\\n\")\n",
    "# combine 点击量特征\n",
    "click_combine_path = feature_path + \"click_num_12dim.csv\"\n",
    "\n",
    "df_qid = pd.read_csv(test_data, names=colnames, header=None,lineterminator=\"\\n\")[\"query_id\"]\n",
    "df = pd.concat([df_qid, df_title], axis=1)\n",
    "\n",
    "# 已保存的文件\n",
    "print(df_qid.shape,df_query.shape,df_title.shape,df_title_groupby.shape)\n",
    "# concat 新文件\n",
    "print(df.shape)\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2A46B55A767441199157FCF5DB40D1AA",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:9: FutureWarning: using a dict on a Series for aggregation\nis deprecated and will be removed in a future version\n  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000000, 12)\ntime: 35min 29s\n"
     ]
    }
   ],
   "source": [
    "click_combine_fout = open(click_combine_path,\"w\")\r\n",
    "aggregation={\r\n",
    "        \"MIN\": lambda x: x.min(skipna=True),\r\n",
    "        \"MAX\":lambda x: x.max(skipna=True),\r\n",
    "        \"MEDIAN\":lambda x: x.median(skipna=True),\r\n",
    "        \"MEAN\":lambda x:x.mean(skipna=True),\r\n",
    "        \"STD\":lambda x:x.std(skipna=True)\r\n",
    "}\r\n",
    "df_tmp = df.groupby(\"query_id\", sort=False)[\"title_num\"].agg(aggregation).reset_index()\r\n",
    "new_df = df.merge(df_tmp,on=['query_id'])[['MIN', 'MAX', 'MEDIAN', 'MEAN', 'STD']]\r\n",
    "hstack_all = np.hstack((\r\n",
    "                    np.array(df_query).reshape(-1,1),\r\n",
    "                    np.array(df_title).reshape(-1,1),\r\n",
    "                    np.array(df_title_groupby),\r\n",
    "                    np.array(new_df),\r\n",
    "                    ))\r\n",
    "print(hstack_all.shape)\r\n",
    "np.savetxt(click_combine_fout,hstack_all,delimiter=\",\",fmt=\"%.5f\")\r\n",
    "click_combine_fout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "76E4212E73514C7C957B4515D26989FE"
   },
   "outputs": [],
   "source": [
    "########################### Rank 余弦相似度特征 ################\n",
    "########################### Rank 余弦相似度特征 ################\n",
    "########################### Rank 余弦相似度特征 ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "3E60498B9E4740EB8AA1CB4A71FE526C",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n(5000000, 2)\n   query_id  consine\n0  14315375  0.76487\n1  14315375  0.83777\n2  14315375  0.83581\n3  14315375  0.84777\n4  14315376  0.99269\ntime: 52.2 s\n"
     ]
    }
   ],
   "source": [
    "#################################### train ##########################\n",
    "feature_path = \"/home/kesci/work/similarity_feature/train/\"\n",
    "train_data = \"/home/kesci/input/bytedance/first-round/train.csv\"\n",
    "consine_feature_path = feature_path + \"consine_sentence_sim.csv\"\n",
    "rank_path = feature_path + \"rank_consine.csv\"\n",
    "\n",
    "samples = 100000000\n",
    "chunksize = 5000000\n",
    "skip_num = int(samples/chunksize) - 1\n",
    "print(skip_num)\n",
    "\n",
    "consine_names = [\"consine\",\"2\",\"3\",\"4\",\"5\",\"6\"]\n",
    "consine_df = pd.read_csv(consine_feature_path,names=consine_names,header=None,lineterminator=\"\\n\")[\"consine\"]\n",
    "df_qid = pd.read_csv(train_data, names=colnames, header=None,\n",
    "                skiprows=chunksize*skip_num,nrows=chunksize,lineterminator=\"\\n\")[\"query_id\"]\n",
    "df_combine = pd.concat([df_qid, consine_df], axis=1)\n",
    "print(df_combine.shape)\n",
    "print(df_combine.head(5))\n",
    "df_combine['Auction_Rank'] = df_combine.groupby('query_id')['consine'].rank(ascending=False)\n",
    "df_combine.to_csv(feature_path+\"rank_consine.csv\",columns=[\"Auction_Rank\"],header=None,index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "702EF3C50F4E4A02A554FF0DB8FBFB71",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n(5000000, 2)\n   query_id  consine\n0  13556849  0.70962\n1  13556849  0.71586\n2  13556849  0.75645\n3  13556849  0.74697\n4  13556849  0.92063\ntime: 53.7 s\n"
     ]
    }
   ],
   "source": [
    "#################################### valid ##########################\n",
    "feature_path = \"/home/kesci/work/similarity_feature/valid/\"\n",
    "train_data = \"/home/kesci/input/bytedance/first-round/train.csv\"\n",
    "consine_feature_path = feature_path + \"consine_sentence_sim.csv\"\n",
    "rank_path = feature_path + \"rank_consine.csv\"\n",
    "\n",
    "samples = 100000000\n",
    "chunksize = 5000000\n",
    "skip_num = int(samples/chunksize) - 2\n",
    "print(skip_num)\n",
    "\n",
    "consine_names = [\"consine\",\"2\",\"3\",\"4\",\"5\",\"6\"]\n",
    "consine_df = pd.read_csv(consine_feature_path,names=consine_names,header=None,lineterminator=\"\\n\")[\"consine\"]\n",
    "df_qid = pd.read_csv(train_data, names=colnames, header=None,\n",
    "                skiprows=chunksize*skip_num,nrows=chunksize,lineterminator=\"\\n\")[\"query_id\"]\n",
    "df_combine = pd.concat([df_qid, consine_df], axis=1)\n",
    "print(df_combine.shape)\n",
    "print(df_combine.head(5))\n",
    "df_combine['Auction_Rank'] = df_combine.groupby('query_id')['consine'].rank(ascending=False)\n",
    "df_combine.to_csv(feature_path+\"rank_consine.csv\",columns=[\"Auction_Rank\"],header=None,index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "824C92F6A2CE4261880EBE9A12850D66",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000000, 2)\n   query_id  consine\n0         1  0.80942\n1         1  0.80083\n2         1  0.76771\n3         1  0.77930\n4         2  0.77215\ntime: 28.8 s\n"
     ]
    }
   ],
   "source": [
    "#################################### test ##########################\n",
    "feature_path = \"/home/kesci/work/similarity_feature/test/\"\n",
    "test_data = \"/home/kesci/input/bytedance/first-round/test.csv\"\n",
    "consine_feature_path = feature_path + \"consine_sentence_sim.csv\"\n",
    "rank_path = feature_path + \"rank_consine.csv\"\n",
    "\n",
    "consine_names = [\"consine\",\"2\",\"3\",\"4\",\"5\",\"6\"]\n",
    "consine_df = pd.read_csv(consine_feature_path,names=consine_names,header=None,lineterminator=\"\\n\")[\"consine\"]\n",
    "df_qid = pd.read_csv(test_data, names=colnames, header=None,lineterminator=\"\\n\")[\"query_id\"]\n",
    "df_combine = pd.concat([df_qid, consine_df], axis=1)\n",
    "print(df_combine.shape)\n",
    "print(df_combine.head(5))\n",
    "df_combine['Auction_Rank'] = df_combine.groupby('query_id')['consine'].rank(ascending=False)\n",
    "df_combine.to_csv(feature_path+\"rank_consine.csv\",columns=[\"Auction_Rank\"],header=None,index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "9F49F742BE6B41B288A73ACFE2FC0505",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 845 µs\n"
     ]
    }
   ],
   "source": [
    "# !cat /home/kesci/work/similarity_feature/train/rank_consine.csv | head -n 10\n",
    "# !cat /home/kesci/work/similarity_feature/valid/rank_consine.csv | head -n 10\n",
    "# !cat /home/kesci/work/similarity_feature/test/rank_consine.csv | head -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "F6216115214D4258BEB065D71EB182D9",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000000\n5000000\n5000000\n8.00000,4.00000,8.00000,12.00000,10.50000,10.25000,1.47902,2.00000,7.00000,4.00000,4.12500,1.55265\n20.00000,1.00000,20.00000,20.00000,20.00000,20.00000,0.00000,1.00000,64.00000,4.00000,11.00000,16.13691\n8.00000,1.00000,8.00000,8.00000,8.00000,8.00000,0.00000,1.00000,3.00000,1.00000,1.25000,0.70711\ntime: 41.8 s\n"
     ]
    }
   ],
   "source": [
    "!cat /home/kesci/work/counting_feature/train/click_num_12dim.csv | wc -l\n",
    "!cat /home/kesci/work/counting_feature/valid/click_num_12dim.csv | wc -l\n",
    "!cat /home/kesci/work/counting_feature/test/click_num_12dim.csv | wc -l\n",
    "\n",
    "!cat /home/kesci/work/counting_feature/train/click_num_12dim.csv | tail -n 1\n",
    "!cat /home/kesci/work/counting_feature/valid/click_num_12dim.csv | tail -n 1\n",
    "!cat /home/kesci/work/counting_feature/test/click_num_12dim.csv | tail -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FD59BF7BE77D4F43A57D8894E76F5927",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000000\n5000000\n5000000\ntime: 5.22 s\n"
     ]
    }
   ],
   "source": [
    "!cat /home/kesci/work/counting_feature/train/basic_feature.csv | wc -l\n",
    "!cat /home/kesci/work/counting_feature/valid/basic_feature.csv | wc -l\n",
    "!cat /home/kesci/work/counting_feature/test/basic_feature.csv | wc -l"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
