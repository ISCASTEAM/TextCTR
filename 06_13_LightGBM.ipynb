{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "trusted": true,
    "collapsed": false,
    "id": "53226C556D654B3A90251327C5880F37",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The klab-autotime extension is already loaded. To reload it, use:\n  %reload_ext klab-autotime\ntime: 4.21 ms\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from numpy import linalg\r\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "from sklearn.model_selection import StratifiedKFold\r\n",
    "import nltk\r\n",
    "import pickle\r\n",
    "from nltk import pos_tag\r\n",
    "from scipy.sparse import hstack, vstack\r\n",
    "from scipy import spatial\r\n",
    "from sklearn.datasets import dump_svmlight_file,load_svmlight_file\r\n",
    "from sklearn.decomposition import TruncatedSVD\r\n",
    "import xgboost as xgb\r\n",
    "import gensim\r\n",
    "from gensim.models import TfidfModel\r\n",
    "from gensim.corpora import Dictionary\r\n",
    "from gensim.models import KeyedVectors\r\n",
    "from sklearn.metrics.pairwise import cosine_similarity\r\n",
    "import lightgbm as lgb\r\n",
    "from sklearn import linear_model\r\n",
    "import gc\r\n",
    "# 显示cell运行时长\r\n",
    "%load_ext klab-autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "trusted": true,
    "collapsed": false,
    "id": "E5B52B083F8648098AC761525C6D4961",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.4 ms\n"
     ]
    }
   ],
   "source": [
    "train_counting_path = \"/home/kesci/work/counting_feature/train/\"\r\n",
    "valid_counting_path = \"/home/kesci/work/counting_feature/valid/\"\r\n",
    "test_counting_path = \"/home/kesci/work/counting_feature/test/\"\r\n",
    "train_sim_path = \"/home/kesci/work/similarity_feature/train/\"\r\n",
    "valid_sim_path = \"/home/kesci/work/similarity_feature/valid/\"\r\n",
    "test_sim_path = \"/home/kesci/work/similarity_feature/test/\"\r\n",
    "\r\n",
    "#统计特征\r\n",
    "counting_train_feature_1 = train_counting_path + \"basic_feature.csv\"\r\n",
    "counting_train_feature_2 = train_counting_path + \"postation.csv\"\r\n",
    "counting_valid_feature_1 = valid_counting_path + \"basic_feature.csv\"\r\n",
    "counting_valid_feature_2 = valid_counting_path + \"postation.csv\"\r\n",
    "counting_test_feature_1 = test_counting_path + \"basic_feature.csv\"\r\n",
    "counting_test_feature_2 = test_counting_path + \"postation.csv\"\r\n",
    "# query,title点击量特征\r\n",
    "click_train_feature_1 = train_counting_path + \"click_num_q.csv\" \r\n",
    "click_train_feature_2 = train_counting_path + \"click_num_t.csv\"\r\n",
    "click_valid_feature_1 = valid_counting_path + \"click_num_q.csv\"\r\n",
    "click_valid_feature_2 = valid_counting_path + \"click_num_t.csv\"\r\n",
    "click_test_feature_1 = test_counting_path + \"click_num_q.csv\"\r\n",
    "click_test_feature_2 = test_counting_path + \"click_num_t.csv\"\r\n",
    "# 点击量特征汇总 06.15\r\n",
    "click_train_feature = train_counting_path + \"click_num_8dim.csv\" \r\n",
    "click_valid_feature = valid_counting_path + \"click_num_8dim.csv\"\r\n",
    "click_test_feature = test_counting_path + \"click_num_8dim.csv\"\r\n",
    "# vector 相似度特征\r\n",
    "sim_train_feature_1 = train_sim_path + \"consine_sentence_sim.csv\"\r\n",
    "sim_train_feature_2 = train_sim_path + \"consine_word_sim.csv\"\r\n",
    "sim_valid_feature_1 = valid_sim_path + \"consine_sentence_sim.csv\"\r\n",
    "sim_valid_feature_2 = valid_sim_path + \"consine_word_sim.csv\"\r\n",
    "sim_test_feature_1 = test_sim_path + \"consine_sentence_sim.csv\"\r\n",
    "sim_test_feature_2 = test_sim_path + \"consine_word_sim.csv\"\r\n",
    "# sentence vector特征\r\n",
    "sentence_train_feature= train_sim_path + \"sentence_vector.csv\"\r\n",
    "sentence_valid_feature = valid_sim_path + \"sentence_vector.csv\"\r\n",
    "sentence_test_feature = test_sim_path + \"sentence_vector.csv\"\r\n",
    "\r\n",
    "\r\n",
    "train_data = \"/home/kesci/input/bytedance/first-round/train.csv\"\r\n",
    "test_data = \"/home/kesci/input/bytedance/first-round/test.csv\"\r\n",
    "model_path = \"/home/kesci/work/GBDT/\"\r\n",
    "colnames = [\"query_id\",\"query\",\"query_title_id\",\"title\",\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "trusted": true,
    "collapsed": false,
    "id": "B53E5220C88148B38DDE55BEF64338A6",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.74 ms\n"
     ]
    }
   ],
   "source": [
    "samples = 100000000\r\n",
    "chunksize =  5000000\r\n",
    "word_dim = 128\r\n",
    "params = {\r\n",
    "    'task': 'train',\r\n",
    "    'boosting_type': 'gbdt',\r\n",
    "    'objective': 'regression_l2',         #binary=logistic; regression=mse\r\n",
    "    'metric': { 'auc', 'binary_logloss'},\r\n",
    "    'num_leaves': 30, \r\n",
    "    \r\n",
    "    'learning_rate': 0.1,  # 每个booster设置相同的学习率\r\n",
    "    'gamma': 0.1,  # 对叶节点个数的惩罚系数\r\n",
    "    'is_training_metric':True,\r\n",
    "    \r\n",
    "    'feature_fraction': 0.9,\r\n",
    "    'bagging_fraction': 0.8,\r\n",
    "    'bagging_freq': 5,\r\n",
    "    # 'verbose': 1\r\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6815D10E8DAB47ED8EEF04DAB07F09F6",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 575 µs\n"
     ]
    }
   ],
   "source": [
    "########### 后500w数据 训练GBDT ##############\n",
    "########### 无样本weight \n",
    "########### 后500w数据 训练GBDT ##############\n",
    "########### 无样本weight \n",
    "########### 后500w数据 训练GBDT ##############\n",
    "########### 无样本weight \n",
    "\n",
    "# 6+5+ 22+10 = 43dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "D74A76A93D00457397A734E7C6A836CE",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000000,)\n(1000000,)\n(5000000,) (1000000,)\ntime: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "################################################# 0. label\n",
    "skip_num = int(samples/chunksize) - 1\n",
    "train_label = pd.read_csv(train_data, names=colnames, header=None,skiprows=chunksize*skip_num,nrows=chunksize,lineterminator=\"\\n\")[\"label\"]\n",
    "skip_num = int(samples/chunksize) - 2\n",
    "nrows_num=1000000\n",
    "valid_label = pd.read_csv(train_data, names=colnames, header=None,skiprows=chunksize*skip_num,nrows=nrows_num,lineterminator=\"\\n\")[\"label\"]\n",
    "\n",
    "print(train_label.shape)\n",
    "print(valid_label.shape)\n",
    "################################################## 0. get sample_weight\n",
    "# def func(x):\n",
    "#     default = 1\n",
    "#     if x == 0:\n",
    "#         default = 0.5\n",
    "#     return default\n",
    "# train_label.to_frame()[\"label\"].apply(func=func)\n",
    "train_weight = train_label.to_frame()[\"label\"].map({0:0.5,1:1})\n",
    "valid_weight = valid_label.to_frame()[\"label\"].map({0:0.5,1:1})\n",
    "print(train_weight.shape,valid_weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "177F0E33953341D39193445287DE37EC",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:  (1000000, 179)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 42.5 s\n"
     ]
    }
   ],
   "source": [
    "################################################# 1. valid_data\r\n",
    "nrows_num = 1000000\r\n",
    "sim_feature_1 = pd.read_csv(sim_valid_feature_1, header=None,nrows=nrows_num,lineterminator=\"\\n\")\r\n",
    "sim_feature_2 = pd.read_csv(sim_valid_feature_2, header=None,nrows=nrows_num,lineterminator=\"\\n\")\r\n",
    "counting_feature_1 = pd.read_csv(counting_valid_feature_1, header=None,nrows=nrows_num,lineterminator=\"\\n\")\r\n",
    "counting_feature_2 = pd.read_csv(counting_valid_feature_2, header=None,nrows=nrows_num,lineterminator=\"\\n\")\r\n",
    "sentence_feature = pd.read_csv(sentence_valid_feature, header=None,nrows=nrows_num,lineterminator=\"\\n\")\r\n",
    "click_feature = pd.read_csv(click_valid_feature, header=None,nrows=nrows_num,lineterminator=\"\\n\")\r\n",
    "combine_feature = np.hstack((\r\n",
    "                         sim_feature_1,\r\n",
    "                         sim_feature_2,\r\n",
    "                         counting_feature_1,\r\n",
    "                         counting_feature_2,\r\n",
    "                         click_feature,\r\n",
    "                         sentence_feature\r\n",
    "                         ))\r\n",
    "print(\"valid: \",combine_feature.shape)\r\n",
    "\r\n",
    "# get lgb data\r\n",
    "dvalid_base = lgb.Dataset(combine_feature, \r\n",
    "                        label=valid_label,\r\n",
    "                        weight=valid_weight)\r\n",
    "del sim_feature_1,sim_feature_2,counting_feature_1,counting_feature_2,combine_feature,sentence_feature\r\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "3CBF569B76694C6E9ABED0A127F0F394",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  (5000000, 179)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3min 9s\n"
     ]
    }
   ],
   "source": [
    "################################################ 2. train_data\r\n",
    "sim_feature_1 = pd.read_csv(sim_train_feature_1, header=None,lineterminator=\"\\n\")\r\n",
    "sim_feature_2 = pd.read_csv(sim_train_feature_2, header=None,lineterminator=\"\\n\")\r\n",
    "counting_feature_1 = pd.read_csv(counting_train_feature_1, header=None,lineterminator=\"\\n\")\r\n",
    "counting_feature_2 = pd.read_csv(counting_train_feature_2, header=None,lineterminator=\"\\n\")\r\n",
    "sentence_feature = pd.read_csv(sentence_train_feature, header=None,lineterminator=\"\\n\")\r\n",
    "click_feature = pd.read_csv(click_train_feature, header=None,lineterminator=\"\\n\")\r\n",
    "combine_feature = np.hstack((\r\n",
    "                         sim_feature_1,\r\n",
    "                         sim_feature_2,\r\n",
    "                         counting_feature_1,\r\n",
    "                         counting_feature_2,\r\n",
    "                         click_feature,\r\n",
    "                         sentence_feature\r\n",
    "                         ))\r\n",
    "print(\"train: \",combine_feature.shape)\r\n",
    "\r\n",
    "# get lgb data\r\n",
    "dtrain_base = lgb.Dataset(combine_feature, \r\n",
    "                        label=train_label,\r\n",
    "                        weight=train_weight)\r\n",
    "del sim_feature_1,sim_feature_2,counting_feature_1,counting_feature_2,combine_feature,sentence_feature\r\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "98FE6B48575A40388E9762D01952E7B8",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 15 rounds.\n[10]\ttrain's auc: 0.693336\ttrain's binary_logloss: 0.62814\tvalid's auc: 0.692516\tvalid's binary_logloss: 0.628875\n[20]\ttrain's auc: 0.695964\ttrain's binary_logloss: 0.61902\tvalid's auc: 0.695167\tvalid's binary_logloss: 0.619891\n[30]\ttrain's auc: 0.697792\ttrain's binary_logloss: 0.616414\tvalid's auc: 0.696876\tvalid's binary_logloss: 0.617418\n[40]\ttrain's auc: 0.699135\ttrain's binary_logloss: 0.615174\tvalid's auc: 0.698056\tvalid's binary_logloss: 0.616308\n[50]\ttrain's auc: 0.700101\ttrain's binary_logloss: 0.614408\tvalid's auc: 0.698867\tvalid's binary_logloss: 0.615653\n[60]\ttrain's auc: 0.700788\ttrain's binary_logloss: 0.613886\tvalid's auc: 0.699389\tvalid's binary_logloss: 0.615235\n[70]\ttrain's auc: 0.701364\ttrain's binary_logloss: 0.61345\tvalid's auc: 0.699809\tvalid's binary_logloss: 0.614932\n[80]\ttrain's auc: 0.701858\ttrain's binary_logloss: 0.6131\tvalid's auc: 0.700142\tvalid's binary_logloss: 0.614688\n[90]\ttrain's auc: 0.702295\ttrain's binary_logloss: 0.612783\tvalid's auc: 0.700451\tvalid's binary_logloss: 0.614466\n[100]\ttrain's auc: 0.702677\ttrain's binary_logloss: 0.612504\tvalid's auc: 0.700693\tvalid's binary_logloss: 0.61428\n[110]\ttrain's auc: 0.703034\ttrain's binary_logloss: 0.612239\tvalid's auc: 0.700908\tvalid's binary_logloss: 0.614127\n[120]\ttrain's auc: 0.703366\ttrain's binary_logloss: 0.612007\tvalid's auc: 0.701091\tvalid's binary_logloss: 0.614043\n[130]\ttrain's auc: 0.703678\ttrain's binary_logloss: 0.611798\tvalid's auc: 0.701274\tvalid's binary_logloss: 0.613914\n[140]\ttrain's auc: 0.703954\ttrain's binary_logloss: 0.611611\tvalid's auc: 0.701413\tvalid's binary_logloss: 0.613815\n[150]\ttrain's auc: 0.704246\ttrain's binary_logloss: 0.61142\tvalid's auc: 0.701587\tvalid's binary_logloss: 0.613711\n[160]\ttrain's auc: 0.704533\ttrain's binary_logloss: 0.61122\tvalid's auc: 0.701746\tvalid's binary_logloss: 0.613605\n[170]\ttrain's auc: 0.704778\ttrain's binary_logloss: 0.611053\tvalid's auc: 0.701841\tvalid's binary_logloss: 0.613539\n[180]\ttrain's auc: 0.705028\ttrain's binary_logloss: 0.610903\tvalid's auc: 0.701961\tvalid's binary_logloss: 0.613458\n[190]\ttrain's auc: 0.705258\ttrain's binary_logloss: 0.61076\tvalid's auc: 0.702059\tvalid's binary_logloss: 0.613395\n[200]\ttrain's auc: 0.705489\ttrain's binary_logloss: 0.610618\tvalid's auc: 0.702157\tvalid's binary_logloss: 0.613374\n[210]\ttrain's auc: 0.705702\ttrain's binary_logloss: 0.610482\tvalid's auc: 0.702237\tvalid's binary_logloss: 0.613362\n[220]\ttrain's auc: 0.705903\ttrain's binary_logloss: 0.610351\tvalid's auc: 0.702306\tvalid's binary_logloss: 0.613323\n[230]\ttrain's auc: 0.706092\ttrain's binary_logloss: 0.610228\tvalid's auc: 0.702353\tvalid's binary_logloss: 0.613335\n[240]\ttrain's auc: 0.706292\ttrain's binary_logloss: 0.610103\tvalid's auc: 0.702422\tvalid's binary_logloss: 0.613244\n[250]\ttrain's auc: 0.706473\ttrain's binary_logloss: 0.609983\tvalid's auc: 0.702445\tvalid's binary_logloss: 0.613273\nDid not meet early stopping. Best iteration is:\n[250]\ttrain's auc: 0.706473\ttrain's binary_logloss: 0.609983\tvalid's auc: 0.702445\tvalid's binary_logloss: 0.613273\ntime: 2h 5min 58s\n"
     ]
    }
   ],
   "source": [
    "################################################ 3. train_model\r\n",
    "\r\n",
    "gbm = lgb.train(params,\r\n",
    "                train_set=dtrain_base,\r\n",
    "                num_boost_round=250,\r\n",
    "                valid_sets=[dvalid_base,dtrain_base],\r\n",
    "                valid_names = [\"valid\",\"train\"],\r\n",
    "                # valid_sets=[lgb_eval],\r\n",
    "                # valid_names = [\"valid\"] ,\r\n",
    "                verbose_eval=10,\r\n",
    "                early_stopping_rounds=15,\r\n",
    "                # feval=cal_qauc\r\n",
    "               )\r\n",
    "pickle.dump(gbm,open(model_path+\"lgb_model.pkl\",\"wb\"))\r\n",
    "\r\n",
    "# Early stopping, best iteration is:\r\n",
    "# [242]\ttrain's binary_logloss: 0.52155\ttrain's auc: 0.695284\t0.523901 valid's auc: 0.6926311\r\n",
    "\r\n",
    "# Early stopping, best iteration is:\r\n",
    "# [217]\ttrain's auc: 0.699427\ttrain's: 0.614691\tvalid's auc: 0.695739\tbinary_logloss: 0.617492"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5AF8085D7A574E1B84A3B4C8ACAF7B1B",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test:  (5000000, 179)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6min 9s\n"
     ]
    }
   ],
   "source": [
    "################################################ 4. predict\n",
    "del dtrain_base,dvalid_base\n",
    "gc.collect()\n",
    "sim_feature_1 = pd.read_csv(sim_test_feature_1, header=None,lineterminator=\"\\n\")\n",
    "sim_feature_2 = pd.read_csv(sim_test_feature_2, header=None,lineterminator=\"\\n\")\n",
    "counting_feature_1 = pd.read_csv(counting_test_feature_1, header=None,lineterminator=\"\\n\")\n",
    "counting_feature_2 = pd.read_csv(counting_test_feature_2, header=None,lineterminator=\"\\n\")\n",
    "sentence_feature = pd.read_csv(sentence_test_feature, header=None,lineterminator=\"\\n\")\n",
    "click_feature = pd.read_csv(click_test_feature, header=None,lineterminator=\"\\n\")\n",
    "\n",
    "combine_feature = np.hstack((\n",
    "                         sim_feature_1,\n",
    "                         sim_feature_2,\n",
    "                         counting_feature_1,\n",
    "                         counting_feature_2,\n",
    "                         click_feature,\n",
    "                         sentence_feature\n",
    "                         ))\n",
    "print(\"test: \",combine_feature.shape)\n",
    "\n",
    "# predict\n",
    "lgb_model = pickle.load(open(model_path+\"lgb_model.pkl\",\"rb\"))\n",
    "predict_ans = lgb_model.predict(combine_feature,ntree_limit=lgb_model.best_iteration)\n",
    "del sim_feature_1,sim_feature_2,counting_feature_1,counting_feature_2,combine_feature,sentence_feature\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "A4118BB6B34344CB80C02A0986E07609",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000000,)\n[0.55691499 0.56963687 0.56857211 0.58509736 0.64476613 0.73858848\n 0.52743986 0.31526443 0.27143407 0.30951374]\ntime: 1.37 ms\n"
     ]
    }
   ],
   "source": [
    "print(predict_ans.shape)\r\n",
    "print(predict_ans[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "B999DFD613FC4977946238DFD11B1BA1",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000000, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 38 s\n"
     ]
    }
   ],
   "source": [
    "colnames = [\"query_id\",\"query\",\"query_title_id\",\"title\"]\r\n",
    "test_df = pd.read_csv(test_data,names=colnames, header=None,lineterminator=\"\\n\")\r\n",
    "test_df[\"pred\"] = predict_ans.reshape(-1)\r\n",
    "print(test_df.shape)\r\n",
    "\r\n",
    "save_path = \"/home/kesci/submit/\"\r\n",
    "test_df.to_csv(save_path+\"500w_sample_weight_sentence_vector_click_more.csv\",columns=[\"query_id\",\"query_title_id\",\"pred\"],header=None,index=None)\r\n",
    "del test_df\r\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "143E558F09CE490C8CE530AAA3E1DFD7",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000000\n1,3,0.5569149929700422\n1,1,0.5696368738374294\n1,4,0.5685721103342345\n1,2,0.5850973625985558\n2,1,0.6447661341325742\ncat: write error: Broken pipe\ntime: 1.4 s\n"
     ]
    }
   ],
   "source": [
    "!cat /home/kesci/submit/500w_sample_weight_sentence_vector_click_more.csv | wc -l\r\n",
    "!cat /home/kesci/submit/500w_sample_weight_sentence_vector_click_more.csv | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "15AA95E8D15548298E4DC399E3E2D4B3",
    "collapsed": false,
    "scrolled": false,
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kesci Submit Tool\nResult File: /home/kesci/submit/500w_sample_weight_sentence_vector_click_more.csv (134.57 MiB)\nUploaded.       \n====================\nSubmit Success.\n{\"Stage\":0,\"Status\":0,\"ShownInHistory\":true,\"IsAucResult\":true,\"Selected\":false,\"_id\":\"5d04cf4fb52341002b806850\",\"Competition\":\"5cc51043f71088002c5b8840\",\"Team\":\"5cf39c212e1a1d002b3916af\",\"UploadDate\":\"2019-06-15T10:58:23.148Z\",\"Final\":true,\"Response\":\"\",\"SubmissionResults\":[],\"IP\":\"52.82.108.66\",\"FingerPrint\":\"\",\"UserAgent\":\"Go-http-client/1.1\",\"ResultFileName\":\"1560596290256dc6b93.csv\",\"ResultFileRealName\":\"500w_sample_weight_sentence_vector_click_more.csv\",\"ResultFileSize\":0,\"ReviewInfos\":[],\"__v\":0}\n\ntime: 14.4 s\n"
     ]
    }
   ],
   "source": [
    "!./kesci_submit -token f3ab22d4d5cb5ea4 -file /home/kesci/submit/500w_sample_weight_sentence_vector_click_more.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "C6FAB7BB2D074DAF909489035432353D",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 864x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/rt_upload/C6FAB7BB2D074DAF909489035432353D/pt56ezf2l4.png\">"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 759 ms\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "import lightgbm as lgb\r\n",
    "model_path = \"/home/kesci/work/GBDT/\"\r\n",
    "lgb_model = pickle.load(open(model_path+\"lgb_model.pkl\",\"rb\"))\r\n",
    "plt.figure(figsize=(12,6))\r\n",
    "lgb.plot_importance(lgb_model, max_num_features=20)\r\n",
    "plt.title(\"Feature importances\")\r\n",
    "plt.show()\r\n",
    "\r\n",
    "# import matplotlib.pyplot as plt\r\n",
    "# model_path = \"/home/kesci/work/GBDT/train/\"\r\n",
    "# lgb_model = pickle.load(open(model_path+\"lgb_model.pkl\",\"rb\"))\r\n",
    "# plt.figure(figsize=(12,6))\r\n",
    "# lgb.plot_importance(lgb_model, max_num_features=20)\r\n",
    "# plt.title(\"Feature importances\")\r\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A2CD1E6393504442B10A7E495CCDBE9E"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "ED356492060A4CC78E171B2E12CA5D44",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000000\n5000000\n5000000\n5000000\n5000000\n5000000\n"
     ]
    }
   ],
   "source": [
    "!cat /home/kesci/work/similarity_feature/train/consine_sentence_sim.csv | wc -l\n",
    "!cat /home/kesci/work/similarity_feature/train/sentence_vector.csv | wc -l\n",
    "\n",
    "!cat /home/kesci/work/similarity_feature/valid/consine_sentence_sim.csv | wc -l\n",
    "!cat /home/kesci/work/similarity_feature/valid/sentence_vector.csv | wc -l\n",
    "\n",
    "!cat /home/kesci/work/similarity_feature/test/consine_sentence_sim.csv | wc -l\n",
    "!cat /home/kesci/work/similarity_feature/test/sentence_vector.csv | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "6BA334B81C1A4D31938B9A4AC6540F6B",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000000\n5000000\n5000000\n"
     ]
    }
   ],
   "source": [
    "!cat /home/kesci/work/similarity_feature/train/consine_word_sim.csv | wc -l\n",
    "!cat /home/kesci/work/similarity_feature/valid/consine_word_sim.csv | wc -l\n",
    "!cat /home/kesci/work/similarity_feature/test/consine_word_sim.csv | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "A8925AB5996A4B038C49CFEDEC6C8A89",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,11202 184 50256,3,11202 184 2346 2527 274 383 34 1033 156 18 50256 19 27 11533 7761 1600 2769\t\ncat: write error: Broken pipe\ntime: 555 ms\n"
     ]
    }
   ],
   "source": [
    "!cat /home/kesci/input/bytedance/first-round/test.csv |head -n 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
