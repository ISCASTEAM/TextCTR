{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "trusted": true,
    "collapsed": false,
    "id": "FB2C0DDC3BB14F95B3FCEF01E90D835F",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from numpy import linalg\r\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "from sklearn.model_selection import StratifiedKFold\r\n",
    "import nltk\r\n",
    "import pickle\r\n",
    "from nltk import pos_tag\r\n",
    "from scipy.sparse import hstack, vstack\r\n",
    "from scipy import spatial\r\n",
    "from sklearn.datasets import dump_svmlight_file,load_svmlight_file\r\n",
    "from sklearn.decomposition import TruncatedSVD\r\n",
    "import xgboost as xgb\r\n",
    "import gensim\r\n",
    "from gensim.models import TfidfModel\r\n",
    "from gensim.corpora import Dictionary\r\n",
    "from gensim.models import KeyedVectors\r\n",
    "from sklearn.metrics.pairwise import cosine_similarity\r\n",
    "import multiprocessing\r\n",
    "import gc\r\n",
    "import logging\r\n",
    "import time\r\n",
    "import os\r\n",
    "# 显示cell运行时长\r\n",
    "%load_ext klab-autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "5672C9EE90924F39845F0AA08A33855D",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.12 ms\n"
     ]
    }
   ],
   "source": [
    "# ###################### 周周星 word2vec #################\n",
    "# class dataGenerator():\n",
    "#     def __init__(self,dirName):\n",
    "#         self.dirName=dirName\n",
    "#     def __iter__(self):\n",
    "#         for fname in os.listdir(self.dirName):\n",
    "#             for line in open(os.path.join(self.dirName,fname)):\n",
    "#                 datas=line.split(',')\n",
    "#                 yield datas[1].split(\" \")+datas[3].split(\" \")\n",
    "                \n",
    "# dir_name = \"/home/kesci/input/bytedance/first-round/\"\n",
    "# vector_size = 128\n",
    "# savedir = \"/home/kesci/work/word2vec/\"\n",
    "# save_name = \"word2vector.model\"\n",
    "\n",
    "# class EpochSaver(gensim.models.callbacks.CallbackAny2Vec):\n",
    "#     '''用于保存模型, 打印损失函数等等'''\n",
    "#     def __init__(self, savedir, save_name):\n",
    "#         os.makedirs(savedir, exist_ok=True)\n",
    "#         self.save_path = os.path.join(savedir, save_name)\n",
    "#         self.epoch = 0\n",
    "#         self.pre_loss = 0\n",
    "#         self.best_loss = 999999999.9\n",
    "#         self.since = time.time()\n",
    "\n",
    "#     def on_epoch_end(self, model):\n",
    "#         self.epoch += 1\n",
    "#         cum_loss = model.get_latest_training_loss() # 返回的是从第一个epoch累计的\n",
    "#         epoch_loss = cum_loss - self.pre_loss\n",
    "#         time_taken = time.time() - self.since\n",
    "#         print(\"Epoch %d, loss: %.2f, time: %dmin %ds\" % \n",
    "#                     (self.epoch, epoch_loss, time_taken//60, time_taken%60))\n",
    "#         if self.best_loss > epoch_loss:\n",
    "#             self.best_loss = epoch_loss\n",
    "#             print(\"Better model. Best loss: %.2f\" % self.best_loss)\n",
    "#             model.save(self.save_path)\n",
    "#             print(\"Model %s save done!\" % self.save_path)\n",
    "\n",
    "#         self.pre_loss = cum_loss\n",
    "#         self.since = time.time()\n",
    "\n",
    "# # 将整个过程分成三步\n",
    "# # 1, 构建模型(不训练)\n",
    "# model_word2vec = gensim.models.Word2Vec(min_count=2, \n",
    "#                                         window=10, \n",
    "#                                         size=vector_size,\n",
    "#                                         workers=63,\n",
    "#                                         batch_words=100000)\n",
    "# # 2, 遍历一遍语料库\n",
    "# since = time.time()\n",
    "# sentences = dataGenerator(dir_name) # 你的sentence迭代器\n",
    "# model_word2vec.build_vocab(sentences, progress_per=20000000)\n",
    "# time_elapsed = time.time() - since\n",
    "# print('Time to build vocab: {:.0f}min {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "# # 3, 训练\n",
    "# since = time.time()\n",
    "# model_word2vec.train(sentences, total_examples=model_word2vec.corpus_count, \n",
    "#                         epochs=8, compute_loss=True, report_delay=60*10, # 每隔10分钟输出一下日志\n",
    "#                         callbacks=[EpochSaver(savedir, save_name)])\n",
    "# time_elapsed = time.time() - since\n",
    "# print('Time to train: {:.0f}min {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "328CAC60A982421DA3FA25A325ADC89F",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.26 ms\n"
     ]
    }
   ],
   "source": [
    "train_data = \"/home/kesci/input/bytedance/first-round/train.csv\"\r\n",
    "test_data = \"/home/kesci/input/bytedance/first-round/test.csv\"\r\n",
    "model_path = \"/home/kesci/work/word2vec/\"\r\n",
    "colnames = [\"query_id\",\"query\",\"query_title_id\",\"title\",\"label\"]\r\n",
    "min_count = 2\r\n",
    "window = 10\r\n",
    "word_dim = 128\r\n",
    "w2viter = 10\r\n",
    "cores = multiprocessing.cpu_count()-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "E99A4B4A139747029FFFAEFCD96BB9C4",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.85 ms\n"
     ]
    }
   ],
   "source": [
    "corpus_file = train_data\r\n",
    "chunksize = 10000\r\n",
    "class getSentences:\r\n",
    "    def __init__(self, filename_list):\r\n",
    "        self.filename_list = filename_list\r\n",
    "    def __iter__(self):\r\n",
    "        for filename in self.filename_list:\r\n",
    "            for df in pd.read_csv(filename,names=colnames, header=None,chunksize=chunksize,lineterminator=\"\\n\"):\r\n",
    "                for row in df.values:\r\n",
    "                    yield  row[1].split(\" \") +  row[3].split(\" \") \r\n",
    "corpus_file_list = [train_data,test_data]\r\n",
    "sentences = getSentences(corpus_file_list)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "650739DFE9694D74A7DB5FEC57DACB63",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4h 40min 23s\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(filename=\"/home/kesci/word2vec.log\",filemode=\"w\",format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\r\n",
    "###### 6.15 重新训练word2vec\r\n",
    "w2vmodel = gensim.models.Word2Vec(sentences,window=window,size=word_dim,workers=cores,min_count=min_count,iter=w2viter)    \r\n",
    "pickle.dump(w2vmodel,open(model_path+\"word2vec_model.pkl\",\"wb\"))\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "F1E191D1805948B885EE2CB567AFF07B",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 642 µs\n"
     ]
    }
   ],
   "source": [
    "############## 加入测试集的训练 word2vec ##############\n",
    "############# 已完成 ############\n",
    "w2vmodel = pickle.load(open(model_path+\"word2vec_model_new.pkl\",\"rb\"))\n",
    "corpus_file = \"/home/kesci/input/bytedance/first-round/test.csv\"\n",
    "sentences_test = getSentences(corpus_file)\n",
    "w2vmodel.build_vocab(sentences_test, update=True)\n",
    "w2vmodel.train(sentences_test,total_examples=w2vmodel.corpus_count,epochs=w2vmodel.epochs)\n",
    "pickle.dump(w2vmodel,open(model_path+\"word2vec_model_new_withTest.pkl\",\"wb\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "983CA3C273CC4C738B31E5C084227A59",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 531 µs\n"
     ]
    }
   ],
   "source": [
    "####### 查看日志\n",
    "# !cat /home/kesci/word2vec.log | tail -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4AECA9246B014AB594DA0D88B2B7AFAD",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.31 s\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/kesci/work/word2vec/\"\n",
    "w2vmodel = pickle.load(open(model_path+\"word2vec_model_new.pkl\",\"rb\"))\n",
    "w2vmodel_test = pickle.load(open(model_path+\"word2vec_model_new_withTest.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "C50C9D544F65414F84B5A15A9400E160",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "842336\n979898\ntime: 1.35 ms\n"
     ]
    }
   ],
   "source": [
    "###### 测试集中有较多的新单词 ######\n",
    "###### 测试集中有较多的新单词 ######\n",
    "###### 测试集中有较多的新单词 ######\n",
    "print(len(w2vmodel.wv.vocab))\n",
    "print(len(w2vmodel_test.wv.vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "C3BA2C35B6384E128D082E3710CD6C99",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9197492003440857"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.44 ms\n"
     ]
    }
   ],
   "source": [
    "########### 对比若干单词的相似度 ##############\n",
    "### 以前的增量训练 导致相似度较低(也有可能是以前的window size设置的较大  or 提高了特征维度大小)  ###\n",
    "from scipy import spatial\n",
    "1 - spatial.distance.cosine(w2vmodel.wv[\"1427\"], w2vmodel_test.wv[\"1427\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ED7B1E2BB6B745C69FDE297F360AFBD2",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.47 s\n"
     ]
    }
   ],
   "source": [
    "######### 对于加入test测试集的w2v 保存dict ######\n",
    "from gensim.models import KeyedVectors\n",
    "# # save dict\n",
    "# save_path = \"/home/kesci/work/word2vec/wordvectors.kv\"\n",
    "# w2vmodel_test = pickle.load(open(model_path+\"word2vec_model_new_withTest.pkl\",\"rb\"))\n",
    "# w2vmodel_test.wv.save(save_path)\n",
    "\n",
    "# 测试\n",
    "save_path = \"/home/kesci/work/word2vec/wordvectors.kv\"\n",
    "w2v_dict = KeyedVectors.load(save_path, mmap='r')\n",
    "len(w2v_dict[\"1427\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6C7FEF67CEDD4CCD8C707F8E97823B1B",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.02 ms\n"
     ]
    }
   ],
   "source": [
    "################### 2019.06.15 重跑similarity特征  ###############################\n",
    "################### 2019.06.15 重跑similarity特征  ###############################\n",
    "################### 2019.06.15 重跑similarity特征   ###############################\n",
    "################### 2019.06.15 重跑similarity特征  ###############################\n",
    "################### 2019.06.15 重跑similarity特征  ###############################\n",
    "################### 2019.06.15 重跑similarity特征  ###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9920B9B106F548CBA2BE0C234B988D90",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "855706"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 10.4 ms\n"
     ]
    }
   ],
   "source": [
    "len(w2vmodel.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "8C02F98FDED5481E8D31E20C6AB5A6CF",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 13.8 s\n"
     ]
    }
   ],
   "source": [
    "colnames = [\"query_id\",\"query\",\"query_title_id\",\"title\",\"label\"]\n",
    "# save w2v dict\n",
    "save_path = \"/home/kesci/work/word2vec/wordvectors.kv\"\n",
    "w2vmodel_test = pickle.load(open(model_path+\"word2vec_model.pkl\",\"rb\"))\n",
    "w2vmodel_test.wv.save(save_path)\n",
    "save_path = \"/home/kesci/work/word2vec/wordvectors.kv\"\n",
    "w2v_dict = KeyedVectors.load(save_path, mmap='r')\n",
    "len(w2v_dict[\"1427\"])\n",
    "# load idf——weight\n",
    "q_idf_weight = pickle.load(open(\"/home/kesci/work/idf/idf_weight_query.pkl\",\"rb\"))\n",
    "t_idf_weight = pickle.load(open(\"/home/kesci/work/idf/idf_weight_title.pkl\",\"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "8F0380A20AEE4B31917C12BA8BF82636",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.45 ms\n"
     ]
    }
   ],
   "source": [
    "def cosine_sim(query, title):\r\n",
    "    l2_query = linalg.norm(query,ord=2,axis=1)\r\n",
    "    l2_title = linalg.norm(title,ord=2,axis=1)\r\n",
    "    eudistance = linalg.norm(query-title,ord=2,axis=1) # 欧式距离 L2\r\n",
    "    manhattan = linalg.norm(query-title,ord=1,axis=1) # 曼哈顿距离 L1\r\n",
    "    chebyshev = linalg.norm(query-title,ord=np.inf,axis=1) # 切比雪夫距离 Max|x1-x2|\r\n",
    "    chebyshev_ = linalg.norm(query-title,ord=-np.inf,axis=1) # Min |x1-x2|\r\n",
    "    my_sqrt_distance = np.sum(np.sqrt(np.absolute(query-title)),axis=1) # sqrt|x1-x2|\r\n",
    "    sim = np.sum(query * title,axis=1)/(l2_query * l2_title)\r\n",
    "    \r\n",
    "    combine_sim = np.hstack((sim.reshape(-1,1),\r\n",
    "                             eudistance.reshape(-1,1),\r\n",
    "                             manhattan.reshape(-1,1),\r\n",
    "                             chebyshev.reshape(-1,1),chebyshev_.reshape(-1,1),\r\n",
    "                             my_sqrt_distance.reshape(-1,1)\r\n",
    "                            ))\r\n",
    "    return combine_sim\r\n",
    "\r\n",
    "#### 1w大小的数据  计算一次\r\n",
    "def concat_sentence(origin_data):\r\n",
    "    q_vstack = \"\"\r\n",
    "    t_vstack = \"\"\r\n",
    "    for row in origin_data.values:\r\n",
    "        # 1.query\r\n",
    "        words = row[1].split(\" \")\r\n",
    "        M = []\r\n",
    "        for w in words:\r\n",
    "            try:\r\n",
    "                M.append(w2v_dict[w] * q_idf_weight[w])\r\n",
    "            except:\r\n",
    "                continue\r\n",
    "        M = np.array(M)\r\n",
    "        if M.shape[0] != 0:\r\n",
    "            q_sentence_vec = M.sum(axis=0) / M.shape[0]\r\n",
    "        else:\r\n",
    "            q_sentence_vec = np.full(word_dim,np.finfo(np.float32).eps)\r\n",
    "        if q_vstack == \"\":\r\n",
    "            q_vstack = q_sentence_vec\r\n",
    "        else:\r\n",
    "             q_vstack = np.vstack((q_vstack,q_sentence_vec))\r\n",
    "        \r\n",
    "        # 2.title\r\n",
    "        words = row[3].split(\" \")\r\n",
    "        M = []\r\n",
    "        for w in words:\r\n",
    "            try:\r\n",
    "                M.append(w2v_dict[w] * t_idf_weight[w])\r\n",
    "            except:\r\n",
    "                continue\r\n",
    "        M = np.array(M)\r\n",
    "        if M.shape[0] != 0:\r\n",
    "            t_sentence_vec = M.sum(axis=0) / M.shape[0]\r\n",
    "        else:\r\n",
    "            t_sentence_vec = np.full(word_dim,np.finfo(np.float32).eps)\r\n",
    "        if t_vstack == \"\":\r\n",
    "            t_vstack = t_sentence_vec\r\n",
    "        else:\r\n",
    "             t_vstack = np.vstack((t_vstack,t_sentence_vec))\r\n",
    "    return q_vstack,t_vstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "1475A23822834FB18D68B7F36576F9B4",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:37: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:55: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 128) (10000, 6)\n(10000, 128) (10000, 6)\n(10000, 128) (10000, 6)\n(10000, 128) (10000, 6)\n(10000, 128) (10000, 6)\ntime: 5h 20min 6s\n"
     ]
    }
   ],
   "source": [
    "############## 后500w 训练集数据 ############\r\n",
    "feature_path = \"/home/kesci/work/similarity_feature/train_idf/\"\r\n",
    "train_data = \"/home/kesci/input/bytedance/first-round/train.csv\"\r\n",
    "# sentece vector of (query-title)\r\n",
    "sentence_vector_path = feature_path+\"sentence_vector.csv\"\r\n",
    "sentence_vec_fout = open(sentence_vector_path,\"w\")\r\n",
    "# 6dim similarity feature\r\n",
    "consine_feature_path = feature_path+\"consine_sentence_sim.csv\"\r\n",
    "sim_qt_fout = open(consine_feature_path,\"w\")\r\n",
    "\r\n",
    "samples = 100000000\r\n",
    "chunksize = 10000\r\n",
    "skip_num = int(samples/chunksize) - 500\r\n",
    "print(skip_num)\r\n",
    "\r\n",
    "for i in range(int(500)):\r\n",
    "    df = pd.read_csv(train_data, names=colnames, header=None,skiprows=chunksize*(skip_num+i),nrows=chunksize,lineterminator=\"\\n\")\r\n",
    "    # 1.get sentece_vector & consine sim\r\n",
    "    q_vstack,t_vstack = concat_sentence(df)\r\n",
    "    combine_sim = cosine_sim(q_vstack,t_vstack)\r\n",
    "    # 2.save\r\n",
    "    np.savetxt(sentence_vec_fout,q_vstack - t_vstack,delimiter=\",\",fmt=\"%.5f\")\r\n",
    "    np.savetxt(sim_qt_fout,combine_sim,delimiter=\",\",fmt=\"%.5f\")\r\n",
    "    # 3.print log\r\n",
    "    if (i+1) % 100 == 0:\r\n",
    "        print((q_vstack - t_vstack).shape,combine_sim.shape)\r\n",
    "    # # 4.gc\r\n",
    "    # del df,q_vstack,t_vstack\r\n",
    "    # gc.collect()\r\n",
    "\r\n",
    "sentence_vec_fout.flush()\r\n",
    "sim_qt_fout.flush()\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "BA9D12B1D8554ED3962F92455B208BE6",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000000\n5000000\n0.78677,67.51521,626.75317,14.74798,0.00117,263.97629\ncat: write error: Broken pipe\n-11.87070,1.43505,-3.70975,-2.85847,-0.23979,-4.70005,-0.00117,-4.35268,-1.77644,8.38602,2.08615,-1.11998,-1.79297,-1.80242,-3.28345,-1.63598,14.74798,5.47955,4.49442,-1.04792,-9.18590,-4.25819,8.44214,7.78709,2.45657,-10.49086,2.80651,-3.31167,2.09307,13.47444,-8.11537,10.25090,-10.84212,12.15462,7.54170,6.33480,-7.32336,-8.45119,8.37578,-5.19715,-0.93039,12.26204,-4.55682,5.84306,-7.00140,0.48155,-4.28800,-3.39937,-5.18955,5.17020,12.94971,-1.39133,-9.43076,0.46021,-1.12013,4.04392,7.16077,-4.52341,7.80053,1.78928,-5.12853,-11.04472,-4.28940,-3.29738,-5.05473,8.37624,13.25131,2.11399,-9.89016,10.99683,6.59390,4.24062,3.71435,1.03263,2.60437,7.22367,-1.98669,-2.07145,-2.66088,4.19020,2.62542,7.46982,1.19146,2.69422,0.31037,3.40124,-4.53363,0.82633,5.69024,-1.76573,-8.01711,-2.39459,0.86877,-7.27101,7.54878,-1.31771,-1.09998,-2.39850,3.23776,-4.21766,7.43775,-3.62145,-6.31628,1.28724,3.65990,-3.69104,4.18867,-5.07926,4.03700,0.59729,5.50399,-1.27490,-9.53605,7.60759,3.31464,-4.06858,6.32394,-1.76592,6.26264,-1.48430,-1.54953,-1.35123,6.56967,-8.89811,-7.53655,-6.82677,-0.62672,2.18691\ncat: write error: Broken pipe\ntime: 11.3 s\n"
     ]
    }
   ],
   "source": [
    "!cat /home/kesci/work/similarity_feature/train_idf/consine_sentence_sim.csv | wc -l\n",
    "!cat /home/kesci/work/similarity_feature/train_idf/sentence_vector.csv | wc -l\n",
    "!cat /home/kesci/work/similarity_feature/train_idf/consine_sentence_sim.csv | head -n 1\n",
    "!cat /home/kesci/work/similarity_feature/train_idf/sentence_vector.csv | head -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DA7954D3B53140B88A7B85303C68D058",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "############## 1000w-500w 验证集数据 ############\r\n",
    "feature_path = \"/home/kesci/work/similarity_feature/valid_idf/\"\r\n",
    "train_data = \"/home/kesci/input/bytedance/first-round/train.csv\"\r\n",
    "# sentece vector of (query-title)\r\n",
    "sentence_vector_path = feature_path+\"sentence_vector.csv\"\r\n",
    "sentence_vec_fout = open(sentence_vector_path,\"w\")\r\n",
    "# 6dim similarity feature\r\n",
    "consine_feature_path = feature_path+\"consine_sentence_sim.csv\"\r\n",
    "sim_qt_fout = open(consine_feature_path,\"w\")\r\n",
    "\r\n",
    "samples = 100000000\r\n",
    "chunksize = 10000\r\n",
    "skip_num = int(samples/chunksize) - 1000\r\n",
    "print(skip_num)\r\n",
    "\r\n",
    "for i in range(int(500)):\r\n",
    "    df = pd.read_csv(train_data, names=colnames, header=None,skiprows=chunksize*(skip_num+i),nrows=chunksize,lineterminator=\"\\n\")\r\n",
    "    # 1.get sentece_vector & consine sim\r\n",
    "    q_vstack,t_vstack = concat_sentence(df)\r\n",
    "    combine_sim = cosine_sim(q_vstack,t_vstack)\r\n",
    "    # 2.save\r\n",
    "    np.savetxt(sentence_vec_fout,q_vstack - t_vstack,delimiter=\",\",fmt=\"%.5f\")\r\n",
    "    np.savetxt(sim_qt_fout,combine_sim,delimiter=\",\",fmt=\"%.5f\")\r\n",
    "\r\n",
    "    # 3.print log\r\n",
    "    if (i+1) % 100 == 0:\r\n",
    "        print((q_vstack - t_vstack).shape,combine_sim.shape)\r\n",
    "    # # 4.gc\r\n",
    "    # del df,q_vstack,t_vstack\r\n",
    "    # gc.collect()\r\n",
    "\r\n",
    "sentence_vec_fout.flush()\r\n",
    "sim_qt_fout.flush()\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24365606610B4C85B3D7EBFF78F1E72B",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!cat /home/kesci/work/similarity_feature/valid_idf/consine_sentence_sim.csv | wc -l\n",
    "!cat /home/kesci/work/similarity_feature/valid_idf/sentence_vector.csv | wc -l\n",
    "!cat /home/kesci/work/similarity_feature/valid_idf/consine_sentence_sim.csv | head -n 1\n",
    "!cat /home/kesci/work/similarity_feature/valid_idf/sentence_vector.csv | head -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8BC90B982FC64A739BB9581226DAA02E",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "############## 测试集数据 ############\r\n",
    "feature_path = \"/home/kesci/work/similarity_feature/test_idf/\"\r\n",
    "test_data = \"/home/kesci/input/bytedance/first-round/test.csv\"\r\n",
    "# sentece vector of (query-title)\r\n",
    "sentence_vector_path = feature_path+\"sentence_vector.csv\"\r\n",
    "sentence_vec_fout = open(sentence_vector_path,\"w\")\r\n",
    "# 6dim similarity feature\r\n",
    "consine_feature_path = feature_path+\"consine_sentence_sim.csv\"\r\n",
    "sim_qt_fout = open(consine_feature_path,\"w\")\r\n",
    "\r\n",
    "chunksize = 10000\r\n",
    "\r\n",
    "for i in range(int(500)):\r\n",
    "    df = pd.read_csv(test_data, names=colnames, header=None,skiprows=chunksize*i,nrows=chunksize,lineterminator=\"\\n\")\r\n",
    "    # 1.get sentece_vector & consine sim\r\n",
    "    q_vstack,t_vstack = concat_sentence(df)\r\n",
    "    combine_sim = cosine_sim(q_vstack,t_vstack)\r\n",
    "    # 2.save\r\n",
    "    np.savetxt(sentence_vec_fout,q_vstack - t_vstack,delimiter=\",\",fmt=\"%.5f\")\r\n",
    "    np.savetxt(sim_qt_fout,combine_sim,delimiter=\",\",fmt=\"%.5f\")\r\n",
    "\r\n",
    "    # 3.print log\r\n",
    "    if (i+1) % 100 == 0:\r\n",
    "        print((q_vstack - t_vstack).shape,combine_sim.shape)\r\n",
    "    # # 4.gc\r\n",
    "    # del df,q_vstack,t_vstack\r\n",
    "    # gc.collect()\r\n",
    "\r\n",
    "sentence_vec_fout.flush()\r\n",
    "sim_qt_fout.flush()\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A686FD80443449D996D4CD37909C8880",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!cat /home/kesci/work/similarity_feature/test_idf/consine_sentence_sim.csv | wc -l\n",
    "!cat /home/kesci/work/similarity_feature/test_idf/sentence_vector.csv | wc -l\n",
    "!cat /home/kesci/work/similarity_feature/test_idf/consine_sentence_sim.csv | head -n 1\n",
    "!cat /home/kesci/work/similarity_feature/test_idf/sentence_vector.csv | head -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "843D94ECCB3A46978CE38D8131D6E5BD",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "########################  query_word & sentence similarity 的计算 ###################\n",
    "########################          min,max,avg,median,std          ###################\n",
    "########################  query_word & sentence similarity 的计算 ###################\n",
    "########################          min,max,avg,median,std          ###################\n",
    "########################  query_word & sentence similarity 的计算 ###################\n",
    "########################          min,max,avg,median,std          ###################\n",
    "########################  query_word & sentence similarity 的计算 ###################\n",
    "########################          min,max,avg,median,std          ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B7E3F4FE28FD4460AEDDBB5AF99597DE"
   },
   "outputs": [],
   "source": [
    "##########################################废弃代码##############################\n",
    "##########################################废弃代码##############################\n",
    "##########################################废弃代码##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "51DCC382692D494B80402E99E1965F71",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from numpy import linalg\r\n",
    "from sklearn.model_selection import StratifiedKFold\r\n",
    "import nltk\r\n",
    "import pickle\r\n",
    "from nltk import pos_tag\r\n",
    "from scipy.sparse import hstack, vstack\r\n",
    "from scipy import spatial\r\n",
    "from sklearn.datasets import dump_svmlight_file,load_svmlight_file\r\n",
    "from sklearn.decomposition import TruncatedSVD\r\n",
    "import xgboost as xgb\r\n",
    "import gensim\r\n",
    "from gensim.models import TfidfModel\r\n",
    "from gensim.corpora import Dictionary\r\n",
    "from gensim.models import KeyedVectors\r\n",
    "from sklearn.metrics.pairwise import cosine_similarity\r\n",
    "import multiprocessing\r\n",
    "import gc\r\n",
    "import logging\r\n",
    "# 显示cell运行时长\r\n",
    "%load_ext klab-autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "44F40FEA4B7E4D538EB0E98BBCFD48BC",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.49 s\n"
     ]
    }
   ],
   "source": [
    "colnames = [\"query_id\",\"query\",\"query_title_id\",\"title\",\"label\"]\n",
    "\n",
    "# # save dict\n",
    "# save_path = \"/home/kesci/work/word2vec/wordvectors.kv\"\n",
    "# w2vmodel_test = pickle.load(open(model_path+\"word2vec_model_new_withTest.pkl\",\"rb\"))\n",
    "# w2vmodel_test.wv.save(save_path)\n",
    "save_path = \"/home/kesci/work/word2vec/wordvectors.kv\"\n",
    "w2v_dict = KeyedVectors.load(save_path, mmap='r')\n",
    "len(w2v_dict[\"1427\"])\n",
    "################ avg word2vec 表征 sentence vector ##############\n",
    "################ avg word2vec 表征 sentence vector ##############\n",
    "################ avg word2vec 表征 sentence vector ##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0C176965B7D04DD28C1639F1BDF10F87",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.22 ms\n"
     ]
    }
   ],
   "source": [
    "def cosine_sim(query, title):\r\n",
    "    l2_query = linalg.norm(query,ord=2,axis=1)\r\n",
    "    l2_title = linalg.norm(title,ord=2,axis=1)\r\n",
    "    eudistance = linalg.norm(query-title,ord=2,axis=1) # 欧式距离 L2\r\n",
    "    manhattan = linalg.norm(query-title,ord=1,axis=1) # 曼哈顿距离 L1\r\n",
    "    chebyshev = linalg.norm(query-title,ord=np.inf,axis=1) # 切比雪夫距离 Max|x1-x2|\r\n",
    "    chebyshev_ = linalg.norm(query-title,ord=-np.inf,axis=1) # Min |x1-x2|\r\n",
    "    my_sqrt_distance = np.sum(np.sqrt(np.absolute(query-title)),axis=1) # sqrt|x1-x2|\r\n",
    "    sim = np.sum(query * title,axis=1)/(l2_query * l2_title)\r\n",
    "    \r\n",
    "    combine_sim = np.hstack((sim.reshape(-1,1),\r\n",
    "                             eudistance.reshape(-1,1),\r\n",
    "                             manhattan.reshape(-1,1),\r\n",
    "                             chebyshev.reshape(-1,1),chebyshev_.reshape(-1,1),\r\n",
    "                             my_sqrt_distance.reshape(-1,1)\r\n",
    "                            ))\r\n",
    "    return combine_sim\r\n",
    "\r\n",
    "#### 1w大小的数据  计算一次\r\n",
    "def concat_sentence(origin_data):\r\n",
    "    q_vstack = \"\"\r\n",
    "    t_vstack = \"\"\r\n",
    "    for row in origin_data.values:\r\n",
    "        # 1.query\r\n",
    "        words = row[1].split(\" \")\r\n",
    "        M = []\r\n",
    "        for w in words:\r\n",
    "            try:\r\n",
    "                M.append(w2v_dict[w])\r\n",
    "            except:\r\n",
    "                continue\r\n",
    "        M = np.array(M)\r\n",
    "        if M.shape[0] != 0:\r\n",
    "            q_sentence_vec = M.sum(axis=0) / M.shape[0]\r\n",
    "        else:\r\n",
    "            q_sentence_vec = np.full(word_dim,np.finfo(np.float32).eps)\r\n",
    "        if q_vstack == \"\":\r\n",
    "            q_vstack = q_sentence_vec\r\n",
    "        else:\r\n",
    "             q_vstack = np.vstack((q_vstack,q_sentence_vec))\r\n",
    "        \r\n",
    "        # 2.title\r\n",
    "        words = row[3].split(\" \")\r\n",
    "        M = []\r\n",
    "        for w in words:\r\n",
    "            try:\r\n",
    "                M.append(w2v_dict[w])\r\n",
    "            except:\r\n",
    "                continue\r\n",
    "        M = np.array(M)\r\n",
    "        if M.shape[0] != 0:\r\n",
    "            t_sentence_vec = M.sum(axis=0) / M.shape[0]\r\n",
    "        else:\r\n",
    "            t_sentence_vec = np.full(word_dim,np.finfo(np.float32).eps)\r\n",
    "        if t_vstack == \"\":\r\n",
    "            t_vstack = t_sentence_vec\r\n",
    "        else:\r\n",
    "             t_vstack = np.vstack((t_vstack,t_sentence_vec))\r\n",
    "    return q_vstack,t_vstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "A635651C301C42EBBEB9512D453A23B7",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:37: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:55: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 128) (10000, 6)\n(10000, 128) (10000, 6)\n(10000, 128) (10000, 6)\n(10000, 128) (10000, 6)\n(10000, 128) (10000, 6)\ntime: 4h 51min 38s\n"
     ]
    }
   ],
   "source": [
    "############## 后1000w-500w 验证集数据 ############\r\n",
    "feature_path = \"/home/kesci/work/similarity_feature/valid/\"\r\n",
    "train_data = \"/home/kesci/input/bytedance/first-round/train.csv\"\r\n",
    "# sentece vector of (query-title)\r\n",
    "sentence_vector_path = feature_path+\"sentence_vector.csv\"\r\n",
    "sentence_vec_fout = open(sentence_vector_path,\"w\")\r\n",
    "# 6dim similarity feature\r\n",
    "consine_feature_path = feature_path+\"consine_sentence_sim.csv\"\r\n",
    "sim_qt_fout = open(consine_feature_path,\"w\")\r\n",
    "\r\n",
    "samples = 100000000\r\n",
    "chunksize = 10000\r\n",
    "skip_num = int(samples/chunksize) - 1000\r\n",
    "print(skip_num)\r\n",
    "\r\n",
    "for i in range(int(500)):\r\n",
    "    df = pd.read_csv(train_data, names=colnames, header=None,skiprows=chunksize*(skip_num+i),nrows=chunksize,lineterminator=\"\\n\")\r\n",
    "    # 1.get sentece_vector & consine sim\r\n",
    "    q_vstack,t_vstack = concat_sentence(df)\r\n",
    "    combine_sim = cosine_sim(q_vstack,t_vstack)\r\n",
    "    # 2.save\r\n",
    "    np.savetxt(sentence_vec_fout,q_vstack - t_vstack,delimiter=\",\",fmt=\"%.5f\")\r\n",
    "    np.savetxt(sim_qt_fout,combine_sim,delimiter=\",\",fmt=\"%.5f\")\r\n",
    "\r\n",
    "    # 3.print log\r\n",
    "    if (i+1) % 100 == 0:\r\n",
    "        print((q_vstack - t_vstack).shape,combine_sim.shape)\r\n",
    "    # # 4.gc\r\n",
    "    # del df,q_vstack,t_vstack\r\n",
    "    # gc.collect()\r\n",
    "\r\n",
    "sentence_vec_fout.flush()\r\n",
    "sim_qt_fout.flush()\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "A04DB8B8F7454BA8867D5D68EB31DC71",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000000\n5000000\ntime: 6.69 s\n"
     ]
    }
   ],
   "source": [
    "!cat /home/kesci/work/similarity_feature/valid/consine_sentence_sim.csv | wc -l\n",
    "!cat /home/kesci/work/similarity_feature/valid/sentence_vector.csv | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E6E059B5DB084E85A5D3A1998AA277B2",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8AC67B1FC6BD40D28847E9847B3DE15C",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C88AB70A67D544E0B714650E2F9CE701"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71FC997EE25C4B2F88642C3BC3D8A834"
   },
   "outputs": [],
   "source": [
    "############################### 废弃代码 #################################\n",
    "############################### 废弃代码 #################################\n",
    "############################### 废弃代码 #################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "44624D284F4C4D4DAA3D008175FA6A34",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  ok_1kw\n1000  ok_1kw\n2000  ok_1kw\n3000  ok_1kw\n4000  ok_1kw\n5000  ok_1kw\n6000  ok_1kw\n7000  ok_1kw\n8000  ok_1kw\n9000  ok_1kw\n100000000\ntime: 22min 53s\n"
     ]
    }
   ],
   "source": [
    "# from collections import Counter\r\n",
    "# model_path = \"/home/kesci/work/word2vec/\"\r\n",
    "# weight_dict = pickle.load(open(model_path+\"idfweight+test.pkl\",\"rb\")) \r\n",
    "# train_data = \"/home/kesci/input/bytedance/first-round/train.csv\"\r\n",
    "# chunksize =  10000\r\n",
    "\r\n",
    "# def topWord(sentence):\r\n",
    "#     tokens = sentence.split(\" \")\r\n",
    "#     q_cnt = Counter()\r\n",
    "#     for word in tokens:\r\n",
    "#         q_cnt[word] +=  weight_dict[word]\r\n",
    "#     return q_cnt.most_common(1)[0][0]\r\n",
    "\r\n",
    "# count = 0\r\n",
    "# key_words = []\r\n",
    "# key_word_count = dict()\r\n",
    "# ket_word_label = dict()\r\n",
    "# for df in pd.read_csv(train_data, names=colnames, header=None,chunksize=chunksize,lineterminator=\"\\n\"):\r\n",
    "#     for row in df.values:\r\n",
    "#         key_word = topWord(row[1])\r\n",
    "#         key_words.append(key_word)\r\n",
    "#         key_word_count[key_word] = key_word_count.get(key_word,0) + 1\r\n",
    "#         if row[-1] == 1:\r\n",
    "#             ket_word_label[key_word] = ket_word_label.get(key_word,0) + 1\r\n",
    "#     if count % 1000 == 0:\r\n",
    "#         print(count,\" ok_1kw\")\r\n",
    "#     count += 1\r\n",
    "\r\n",
    "# print(len(key_words))\r\n",
    "\r\n",
    "# path = \"/home/kesci/work/ctr/\"\r\n",
    "# pickle.dump(key_words,open(path+\"key_words\",\"wb\"))\r\n",
    "# pickle.dump(key_word_count,open(path+\"key_word_count\",\"wb\"))\r\n",
    "# pickle.dump(ket_word_label,open(path+\"ket_word_label\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "53AEDE6068A1450A8451523EB024731D",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 709 ms\n"
     ]
    }
   ],
   "source": [
    "# for key,val in ket_word_label.items():\n",
    "#     ket_word_label[key] = float(val)/key_word_count[key]\n",
    "# pickle.dump(ket_word_label,open(path+\"key_word_origin_ctr\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "B097FB150C144ACCA49A3038DDD358B3",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000000\ntime: 2min 44s\n"
     ]
    }
   ],
   "source": [
    "# key_word_ctr = pickle.load(open(path+\"key_word_origin_ctr\",\"rb\"))\n",
    "# feature_ctr_path = path + \"feature_ctr.csv\"\n",
    "\n",
    "# avg = 0.29\n",
    "# with open(feature_ctr_path,\"w\") as fout:\n",
    "#     for word in key_words:\n",
    "#         ctr = key_word_ctr.get(word,avg)\n",
    "#         fout.write(str(ctr)+\"\\n\")\n",
    "\n",
    "# !cat /home/kesci/work/ctr/feature_ctr.csv | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "90D156FFE668448C833E5005B1B50710",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31769722814498935\ncat: write error: Broken pipe\ntime: 882 ms\n"
     ]
    }
   ],
   "source": [
    "# !cat /home/kesci/work/ctr/feature_ctr.csv | head -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "3A5B83259BE947779F0CABA01AD9690B",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  ok_1kw\n5000000\ntime: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "# from collections import Counter\r\n",
    "# model_path = \"/home/kesci/work/word2vec/\"\r\n",
    "# weight_dict = pickle.load(open(model_path+\"idfweight+test.pkl\",\"rb\")) \r\n",
    "# test_data = \"/home/kesci/input/bytedance/first-round/test.csv\"\r\n",
    "# chunksize =  10000\r\n",
    "\r\n",
    "# def topWord(sentence):\r\n",
    "#     tokens = sentence.split(\" \")\r\n",
    "#     q_cnt = Counter()\r\n",
    "#     for word in tokens:\r\n",
    "#         q_cnt[word] +=  weight_dict[word]\r\n",
    "#     return q_cnt.most_common(1)\r\n",
    "\r\n",
    "# count = 0\r\n",
    "# key_words_unigram_test = []\r\n",
    "# for df in pd.read_csv(test_data, names=colnames, header=None,chunksize=chunksize,lineterminator=\"\\n\"):\r\n",
    "#     for row in df.values:\r\n",
    "#         key_words_unigram_test.append(topWord(row[1])[0][0])\r\n",
    "#     if count % 1000 == 0:\r\n",
    "#         print(count,\" ok_1kw\")\r\n",
    "#     count += 1\r\n",
    "\r\n",
    "# print(len(key_words_unigram_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5E21E0383BC340E88F4891359DF455F0",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176124\ntime: 361 ms\n"
     ]
    }
   ],
   "source": [
    "# key_words_unigram_test_set = set(key_words_unigram_test)\n",
    "# print(len(key_words_unigram_test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "F4853BB84A414FAE989A41373B83A396",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.14 ms\n"
     ]
    }
   ],
   "source": [
    "# repeat_q = 0\n",
    "# for test_q in key_words_unigram_test:\n",
    "#     if test_q in key_words_unigram_set:\n",
    "#         repeat_q += 1\n",
    "        \n",
    "# print(repeat_q)\n",
    "\n",
    "# ######### 统计tf_idf 49w的单词能在训练集中出现 (训练集大于50w关键单词  未知是否有良好的区分度)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "48079C7FA05147ACAA49008B08DAABF2",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510583\n510583\ntime: 21min 54s\n"
     ]
    }
   ],
   "source": [
    "# ############ 关键单词的ctr值(观察区分度)\n",
    "# from collections import Counter\n",
    "# model_path = \"/home/kesci/work/word2vec/\"\n",
    "# weight_dict = pickle.load(open(model_path+\"idfweight+test.pkl\",\"rb\")) \n",
    "# train_data = \"/home/kesci/input/bytedance/first-round/train.csv\"\n",
    "# chunksize =  10000\n",
    "\n",
    "# def topWord(sentence):\n",
    "#     tokens = sentence.split(\" \")\n",
    "#     q_cnt = Counter()\n",
    "#     for word in tokens:\n",
    "#         q_cnt[word] +=  weight_dict[word]\n",
    "#     return q_cnt.most_common(1)[0][0]\n",
    "\n",
    "# key_word_count = dict()\n",
    "# ket_word_label = dict()\n",
    "# for df in pd.read_csv(train_data, names=colnames, header=None,chunksize=chunksize,lineterminator=\"\\n\"):\n",
    "#     for row in df.values:\n",
    "#         key_word = topWord(row[1])\n",
    "#         key_word_count[key_word] = key_word_count.get(key_word,0) + 1\n",
    "#         if row[-1] == 1:\n",
    "#             ket_word_label[key_word] = ket_word_label.get(key_word,0) + 1\n",
    "\n",
    "# print(len(key_word_count))\n",
    "# print(len(ket_word_label))\n",
    "\n",
    "# for key,val in ket_word_label.items():\n",
    "#     ket_word_label[key] = val/key_word_count[key]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "A782DC9DDD2F445D8D01A7E89FDBF276",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185670\ntime: 104 ms\n"
     ]
    }
   ],
   "source": [
    "# ########## 置信度 单词出现次数（浏览数）大于一定的阈值\n",
    "# count = 0\n",
    "# for key,val in key_word_count.items():\n",
    "#     if val <= 10:\n",
    "#         count += 1\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "9E7D5C63A6B04C0D9ED480E533ECAB2F",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3858060100070629\ntime: 236 ms\n"
     ]
    }
   ],
   "source": [
    "# ########### 平均ctr\n",
    "# ctr_all = 0\n",
    "# count = 0\n",
    "# for key,val in ket_word_label.items():\n",
    "#     if key_word_count[key] <= 5:\n",
    "#         ctr_all += val\n",
    "#         count += 1\n",
    "# print(ctr_all/count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "C996305EA667492C81FFBFF03ED3BBE6",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13769\ntime: 107 ms\n"
     ]
    }
   ],
   "source": [
    "# count = 0\n",
    "# for key,val in ket_word_label.items():\n",
    "#     if val <= 0.1:\n",
    "#         count += 1\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "C5603AB32E8C4E21857C72D99AA344D5",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 462 µs\n"
     ]
    }
   ],
   "source": [
    "# repeat_q = 0\r\n",
    "# repeat_t = 0\r\n",
    "# count = 0\r\n",
    "# for df in pd.read_csv(test_data, names=colnames, header=None,chunksize=chunksize,lineterminator=\"\\n\"):\r\n",
    "#     tmp_q = \r\n",
    "#     df['query'].values\r\n",
    "#     tmp_t = df['title'].values\r\n",
    "#     for q in tmp_q:\r\n",
    "#         if q in query_set:\r\n",
    "#             repeat_q += 1\r\n",
    "            \r\n",
    "#     for t in tmp_t:\r\n",
    "#         if t in title_set:\r\n",
    "#             repeat_t += 1\r\n",
    "#     del tmp_q\r\n",
    "#     del tmp_t\r\n",
    "#     gc.collect()\r\n",
    "    \r\n",
    "#     print(str(count) + \" ok\")\r\n",
    "#     if count >= 9:\r\n",
    "#         break\r\n",
    "#     count += 1\r\n",
    "    \r\n",
    "# print(repeat_q,repeat_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "B00D384FA55F499CA6C155834B653BB0",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\nok\nok\nok\nok\nok\nok\nok\nok\nok\n2384672 21517533 28889730 47208065\ntime: 25min 40s\n"
     ]
    }
   ],
   "source": [
    "# ###################  query  长度测试\n",
    "# count1 = 0\n",
    "# count2 = 0\n",
    "# count3 = 0\n",
    "# count4 = 0\n",
    "# chunksize = 10000000\n",
    "# for df in pd.read_csv(train_data, names=colnames, header=None,chunksize=chunksize,lineterminator=\"\\n\"):\n",
    "#     for row in df.values:\n",
    "#         tokens = row[1].split(\" \")\n",
    "#         if len(tokens) == 1:\n",
    "#             count1 += 1\n",
    "#         elif len(tokens) == 2:\n",
    "#             count2 += 1\n",
    "#         elif len(tokens) == 3:\n",
    "#             count3 += 1\n",
    "#         else:\n",
    "#             count4 += 1\n",
    "#     print(\"ok\")        \n",
    "# print(count1,count2,count3,count4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "1205F4A6982F421092D840C03EB79343",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\nok\nok\nok\nok\n153582 1020210 1423634 2402574\ntime: 19.1 s\n"
     ]
    }
   ],
   "source": [
    "# ###################  query  长度测试\n",
    "# test_data = \"/home/kesci/input/bytedance/first-round/test.csv\"\n",
    "# count1 = 0\n",
    "# count2 = 0\n",
    "# count3 = 0\n",
    "# count4 = 0\n",
    "# chunksize = 1000000\n",
    "# for df in pd.read_csv(test_data, names=colnames, header=None,chunksize=chunksize,lineterminator=\"\\n\"):\n",
    "#     for row in df.values:\n",
    "#         tokens = row[1].split(\" \")\n",
    "#         if len(tokens) == 1:\n",
    "#             count1 += 1\n",
    "#         elif len(tokens) == 2:\n",
    "#             count2 += 1\n",
    "#         elif len(tokens) == 3:\n",
    "#             count3 += 1\n",
    "#         else:\n",
    "#             count4 += 1\n",
    "#     print(\"ok\")        \n",
    "# print(count1,count2,count3,count4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "B51E33E7FF6C42948A6B1B4F1F8CEAD5",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 584 ms\n"
     ]
    }
   ],
   "source": [
    "# ####################### unigram 测试   2019 06 10####################\n",
    "# ####################### unigram 测试   2019 06 10####################\n",
    "# ####################### unigram 测试   2019 06 10####################\n",
    "\n",
    "# from collections import Counter\n",
    "# colnames = [\"query_id\",\"query\",\"query_title_id\",\"title\",\"label\"]\n",
    "# train_data = \"/home/kesci/input/bytedance/first-round/train.csv\"\n",
    "# weight_dict = pickle.load(open(\"/home/kesci/work/ctr/idf_weight_only_train.pkl\",\"rb\")) \n",
    "# chunksize =  10000\n",
    "# def topWord(sentence,ngram=1):\n",
    "#     tokens = sentence.split(\" \")\n",
    "#     q_cnt = Counter()\n",
    "#     for word in tokens:\n",
    "#         q_cnt[word] +=  weight_dict[word] # 对于test集中 可能有很多query不在weight_dict\n",
    "#     keywords = q_cnt.most_common(ngram)\n",
    "#     return \"_\".join([word[0] for word in keywords])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "98F3E18CE92A4A7A8B8B0A544FA71811",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\n8000\n9000\ntime: 25min 19s\n"
     ]
    }
   ],
   "source": [
    "# key_words_unigram = []\n",
    "# count = 0\n",
    "# for df in pd.read_csv(train_data, names=colnames, header=None,chunksize=chunksize,lineterminator=\"\\n\"):\n",
    "#     for row in df.values:\n",
    "#         key_words_unigram.append(topWord(row[1]))\n",
    "#     if count%1000 == 0:\n",
    "#         print(count)\n",
    "#     count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7A12188C368C409ABEA14524AB2B44FC",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000000\n550485\ntime: 26.8 s\n"
     ]
    }
   ],
   "source": [
    "# key_words_unigram_set = set(key_words_unigram)\n",
    "# print(len(key_words_unigram))\n",
    "# print(len(key_words_unigram_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "CD2F39F18B0543CCA2E07E81CF586677",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n100\n200\n300\n400\ntime: 6min 26s\n"
     ]
    }
   ],
   "source": [
    "# from collections import Counter\n",
    "# colnames = [\"query_id\",\"query\",\"query_title_id\",\"title\",\"label\"]\n",
    "# model_path = \"/home/kesci/work/word2vec/\"\n",
    "# test_data = \"/home/kesci/input/bytedance/first-round/test.csv\"\n",
    "# chunksize =  10000\n",
    "\n",
    "# def topWord(sentence,ngram=1):\n",
    "#     tokens = sentence.split(\" \")\n",
    "#     q_cnt = Counter()\n",
    "#     for word in tokens:\n",
    "#         q_cnt[word] +=  weight_dict.get(word,1)\n",
    "#     keywords = q_cnt.most_common(ngram)\n",
    "#     return \"_\".join([word[0] for word in keywords])\n",
    "    \n",
    "# key_words_unigram_test = []\n",
    "# count = 0\n",
    "# for df in pd.read_csv(test_data, names=colnames, header=None,chunksize=chunksize,lineterminator=\"\\n\"):\n",
    "#     for row in df.values:\n",
    "#         key_words_unigram_test.append(topWord(row[1]))\n",
    "#     if count%100 == 0:\n",
    "#         print(count)\n",
    "#     count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "DBB0A9F0B57A4F498341279AD143A264",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000000\n176662\n173289\ntime: 702 ms\n"
     ]
    }
   ],
   "source": [
    "# key_words_unigram_test_set = set(key_words_unigram_test)\n",
    "# print(len(key_words_unigram_test))\n",
    "# print(len(key_words_unigram_test_set))\n",
    "\n",
    "# count = 0\n",
    "# for q in key_words_unigram_test_set:\n",
    "#     if q in key_words_unigram_set:\n",
    "#         count += 1\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CFDD9C41CD1F4DB98FC6E2DA31107EC1"
   },
   "outputs": [],
   "source": [
    "############# 训练集计算unigram的点击率特征\n",
    "for i in ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "A6B097A1605B411981C052B7E20078BD",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.01 s\n"
     ]
    }
   ],
   "source": [
    "# ######################  bigram 测试 ##################\n",
    "# ######################  bigram 测试 ##################\n",
    "\n",
    "# from collections import Counter\n",
    "# weight_dict = pickle.load(open(\"/home/kesci/work/ctr/idf_weight_only_train.pkl\",\"rb\")) \n",
    "# def topWord(sentence,ngram=2):\n",
    "#     tokens = sentence.split(\" \")\n",
    "#     q_cnt = Counter()\n",
    "#     for word in tokens:\n",
    "#         q_cnt[word] +=  weight_dict[word] # 对于test集中 可能有很多query不在weight_dict\n",
    "#     keywords = q_cnt.most_common(ngram)\n",
    "#     return \"_\".join([word[0] for word in keywords])\n",
    "    \n",
    "# topWord(\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "58452CDF164242268321B8B88818C835",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 639 µs\n"
     ]
    }
   ],
   "source": [
    "# ######################  bigram 测试 ##################\n",
    "# ######################  bigram 测试 ##################\n",
    "# from collections import Counter\n",
    "# colnames = [\"query_id\",\"query\",\"query_title_id\",\"title\",\"label\"]\n",
    "# model_path = \"/home/kesci/work/word2vec/\"\n",
    "# train_data = \"/home/kesci/input/bytedance/first-round/train.csv\"\n",
    "# chunksize =  10000\n",
    "\n",
    "# key_words_bigram = []\n",
    "# count = 0\n",
    "# for df in pd.read_csv(train_data, names=colnames, header=None,chunksize=chunksize,lineterminator=\"\\n\"):\n",
    "#     for row in df.values:\n",
    "#         key_words_bigram.append(topWord(row[1]))\n",
    "#     if count%1000 == 0:\n",
    "#         print(count)\n",
    "#     count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "49CA13C389EE434E8D6F8823B041138E",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 401 µs\n"
     ]
    }
   ],
   "source": [
    "# key_words_bigram_set = set(key_words_bigram)\n",
    "# print(len(key_words_bigram))\n",
    "# print(len(key_words_bigram_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "A426E167A7724E0387AA3667C5A7AF4F",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 374 µs\n"
     ]
    }
   ],
   "source": [
    "# pickle.dump(key_words_bigram_set,open(\"/home/kesci/work/ctr/key_words_bigram_set.keywords\",\"wb\"))\n",
    "# pickle.dump(key_words_bigram,open(\"/home/kesci/work/ctr/key_words_bigram.keywords\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5F4D87B97661429A80C36C2662B32C38",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 875 µs\n"
     ]
    }
   ],
   "source": [
    "# from collections import Counter\n",
    "# colnames = [\"query_id\",\"query\",\"query_title_id\",\"title\",\"label\"]\n",
    "# model_path = \"/home/kesci/work/word2vec/\"\n",
    "# test_data = \"/home/kesci/input/bytedance/first-round/test.csv\"\n",
    "# chunksize =  10000\n",
    "\n",
    "# def topWord(sentence,ngram=2):\n",
    "#     tokens = sentence.split(\" \")\n",
    "#     q_cnt = Counter()\n",
    "#     for word in tokens:\n",
    "#         # q_cnt[word] +=  weight_dict[word] # 对于test集中 可能有很多query不在weight_dict当中\n",
    "#         q_cnt[word] +=  weight_dict.get(word,1)\n",
    "#     keywords = q_cnt.most_common(ngram)\n",
    "#     return \"_\".join([word[0] for word in keywords])\n",
    "    \n",
    "# key_words_bigram_test = []\n",
    "# count = 0\n",
    "# for df in pd.read_csv(test_data, names=colnames, header=None,chunksize=chunksize,lineterminator=\"\\n\"):\n",
    "#     for row in df.values:\n",
    "#         key_words_bigram_test.append(topWord(row[1]))\n",
    "#     if count%100 == 0:\n",
    "#         print(count)\n",
    "#     count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "D1051C1220AE432FA14F0390AC42EC80",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.59 s\n"
     ]
    }
   ],
   "source": [
    "# key_words_bigram_test_set = set(key_words_bigram_test)\n",
    "# pickle.dump(key_words_bigram_test_set,open(\"/home/kesci/work/ctr/key_words_bigram_test_set.keywords\",\"wb\"))\n",
    "# pickle.dump(key_words_bigram_test,open(\"/home/kesci/work/ctr/key_words_bigram_test.keywords\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "59FA7EE8D7C944D7A28FE14865B13D7C",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000000\n1619257\ntime: 1.03 s\n"
     ]
    }
   ],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "251389A145A64A7E89D3BE0A7D208A67",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000000\n816096\n383996\n801342\ntime: 2.75 s\n"
     ]
    }
   ],
   "source": [
    "# print(len(key_words_bigram_test))\n",
    "# key_words_bigram_test_set = set(key_words_bigram_test)\n",
    "# print(len(key_words_bigram_test_set))\n",
    "\n",
    "# count = 0\n",
    "# double = 0\n",
    "# for q in key_words_bigram_test_set:\n",
    "#     words = q.split(\"_\")\n",
    "#     tmp_q = \"\"\n",
    "#     if len(words) == 2:\n",
    "#         tmp_q = words[1] + \"_\" + words[0]\n",
    "#         double += 1\n",
    "#     if q in key_words_bigram_set or tmp_q in key_words_bigram_set:\n",
    "#         count += 1\n",
    "\n",
    "# print(count)\n",
    "# print(double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F4C6B6071A4F4A21864484ECEC0CE798"
   },
   "outputs": [],
   "source": [
    "# ############ 训练集800w的query关键词的ctr的区分度\n",
    "# from collections import Counter\n",
    "# colnames = [\"query_id\",\"query\",\"query_title_id\",\"title\",\"label\"]\n",
    "# model_path = \"/home/kesci/work/word2vec/\"\n",
    "# train_data = \"/home/kesci/input/bytedance/first-round/train.csv\"\n",
    "# chunksize =  10000\n",
    "\n",
    "# key_words_bigram = []\n",
    "# count = 0\n",
    "# for df in pd.read_csv(train_data, names=colnames, header=None,chunksize=chunksize,lineterminator=\"\\n\"):\n",
    "#     for row in df.values:\n",
    "#         if row[-1] == 1:\n",
    "            \n",
    "#     if count%1000 == 0:\n",
    "#         print(count)\n",
    "#     count += 1\n",
    "    \n",
    "# ##  其他思路 使用余弦相似度寻找最相近的train-query的匹配"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
