{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "trusted": true,
    "collapsed": false,
    "id": "AABFB8BB338349B2A4BD18C66868AABA",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from numpy import linalg\r\n",
    "from sklearn.model_selection import StratifiedKFold\r\n",
    "import nltk\r\n",
    "import pickle\r\n",
    "from nltk import pos_tag\r\n",
    "from scipy.sparse import hstack, vstack\r\n",
    "from scipy import spatial\r\n",
    "from sklearn.datasets import dump_svmlight_file,load_svmlight_file\r\n",
    "from sklearn.decomposition import TruncatedSVD\r\n",
    "import xgboost as xgb\r\n",
    "import gensim\r\n",
    "from gensim.models import TfidfModel\r\n",
    "from gensim.corpora import Dictionary\r\n",
    "from gensim.models import KeyedVectors\r\n",
    "from sklearn.metrics.pairwise import cosine_similarity\r\n",
    "import multiprocessing\r\n",
    "import gc\r\n",
    "import logging\r\n",
    "# 显示cell运行时长\r\n",
    "%load_ext klab-autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "trusted": true,
    "collapsed": false,
    "id": "889052BA237747DC9D4DBDDD65F7BBA8",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.7 s\n"
     ]
    }
   ],
   "source": [
    "colnames = [\"query_id\",\"query\",\"query_title_id\",\"title\",\"label\"]\n",
    "\n",
    "# load dict\n",
    "save_path = \"/home/kesci/work/word2vec/wordvectors.kv\"\n",
    "w2v_dict = KeyedVectors.load(save_path, mmap='r')\n",
    "len(w2v_dict[\"1427\"])\n",
    "################ avg word2vec 表征 sentence vector ##############\n",
    "################ avg word2vec 表征 sentence vector ##############\n",
    "################ avg word2vec 表征 sentence vector ##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "30C00E87E0C546DA8AC0AA57D47A0CD5",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.82 s\n"
     ]
    }
   ],
   "source": [
    "# load idf——weight\n",
    "q_idf_weight = pickle.load(open(\"/home/kesci/work/idf/idf_weight_query.pkl\",\"rb\"))\n",
    "t_idf_weight = pickle.load(open(\"/home/kesci/work/idf/idf_weight_title.pkl\",\"rb\"))\n",
    "################ tf-idf加权 word2vec 表征 sentence vector ##############\n",
    "################ tf-idf加权 word2vec 表征 sentence vector ##############\n",
    "################ tf-idf加权 word2vec 表征 sentence vector ##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "trusted": true,
    "collapsed": false,
    "id": "93CBA9A47E6F4F238F07739EAEB4BA5C",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.92 ms\n"
     ]
    }
   ],
   "source": [
    "def cosine_sim(query, title):\r\n",
    "    l2_query = linalg.norm(query,ord=2,axis=1)\r\n",
    "    l2_title = linalg.norm(title,ord=2,axis=1)\r\n",
    "    eudistance = linalg.norm(query-title,ord=2,axis=1) # 欧式距离 L2\r\n",
    "    manhattan = linalg.norm(query-title,ord=1,axis=1) # 曼哈顿距离 L1\r\n",
    "    chebyshev = linalg.norm(query-title,ord=np.inf,axis=1) # 切比雪夫距离 Max|x1-x2|\r\n",
    "    chebyshev_ = linalg.norm(query-title,ord=-np.inf,axis=1) # Min |x1-x2|\r\n",
    "    my_sqrt_distance = np.sum(np.sqrt(np.absolute(query-title)),axis=1) # sqrt|x1-x2|\r\n",
    "    sim = np.sum(query * title,axis=1)/(l2_query * l2_title)\r\n",
    "    \r\n",
    "    combine_sim = np.hstack((sim.reshape(-1,1),\r\n",
    "                             eudistance.reshape(-1,1),\r\n",
    "                             manhattan.reshape(-1,1),\r\n",
    "                             chebyshev.reshape(-1,1),chebyshev_.reshape(-1,1),\r\n",
    "                             my_sqrt_distance.reshape(-1,1)\r\n",
    "                            ))\r\n",
    "    return combine_sim\r\n",
    "\r\n",
    "#### 1w大小的数据  计算一次\r\n",
    "def concat_sentence(origin_data):\r\n",
    "    q_vstack = \"\"\r\n",
    "    t_vstack = \"\"\r\n",
    "    for row in origin_data.values:\r\n",
    "        # 1.query\r\n",
    "        words = row[1].split(\" \")\r\n",
    "        M = []\r\n",
    "        for w in words:\r\n",
    "            try:\r\n",
    "                M.append(w2v_dict[w] * q_idf_weight[w])\r\n",
    "            except:\r\n",
    "                continue\r\n",
    "        M = np.array(M)\r\n",
    "        if M.shape[0] != 0:\r\n",
    "            q_sentence_vec = M.sum(axis=0) / M.shape[0]\r\n",
    "        else:\r\n",
    "            q_sentence_vec = np.full(word_dim,np.finfo(np.float32).eps)\r\n",
    "        if q_vstack == \"\":\r\n",
    "            q_vstack = q_sentence_vec\r\n",
    "        else:\r\n",
    "             q_vstack = np.vstack((q_vstack,q_sentence_vec))\r\n",
    "        \r\n",
    "        # 2.title\r\n",
    "        words = row[3].split(\" \")\r\n",
    "        M = []\r\n",
    "        for w in words:\r\n",
    "            try:\r\n",
    "                M.append(w2v_dict[w] * t_idf_weight[w])\r\n",
    "            except:\r\n",
    "                continue\r\n",
    "        M = np.array(M)\r\n",
    "        if M.shape[0] != 0:\r\n",
    "            t_sentence_vec = M.sum(axis=0) / M.shape[0]\r\n",
    "        else:\r\n",
    "            t_sentence_vec = np.full(word_dim,np.finfo(np.float32).eps)\r\n",
    "        if t_vstack == \"\":\r\n",
    "            t_vstack = t_sentence_vec\r\n",
    "        else:\r\n",
    "             t_vstack = np.vstack((t_vstack,t_sentence_vec))\r\n",
    "    return q_vstack,t_vstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "trusted": true,
    "collapsed": false,
    "id": "CB3D87C248AF414A93B75C38C82A91FF",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:37: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:55: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 128)\n(10000, 128)\n(10000, 128)\n(10000, 128)\n(10000, 128)\ntime: 4h 52min 5s\n"
     ]
    }
   ],
   "source": [
    "# ############## 后500w 训练集数据 ############\r\n",
    "# feature_path = \"/home/kesci/work/similarity_feature/train_idf/\"\r\n",
    "# train_data = \"/home/kesci/input/bytedance/first-round/train.csv\"\r\n",
    "# # sentece vector of (query-title)\r\n",
    "# sentence_vector_path = feature_path+\"sentence_vector.csv\"\r\n",
    "# sentence_vec_fout = open(sentence_vector_path,\"w\")\r\n",
    "# # 6dim similarity feature\r\n",
    "# consine_feature_path = feature_path+\"consine_sentence_sim.csv\"\r\n",
    "# sim_qt_fout = open(consine_feature_path,\"w\")\r\n",
    "\r\n",
    "# samples = 100000000\r\n",
    "# chunksize = 10000\r\n",
    "# skip_num = int(samples/chunksize) - 500\r\n",
    "# print(skip_num)\r\n",
    "\r\n",
    "# for i in range(int(500)):\r\n",
    "#     df = pd.read_csv(train_data, names=colnames, header=None,skiprows=chunksize*(skip_num+i),nrows=chunksize,lineterminator=\"\\n\")\r\n",
    "#     # 1.get sentece_vector & consine sim\r\n",
    "#     q_vstack,t_vstack = concat_sentence(df)\r\n",
    "#     combine_sim = cosine_sim(q_vstack,t_vstack)\r\n",
    "#     # 2.save\r\n",
    "#     np.savetxt(sentence_vec_fout,q_vstack - t_vstack,delimiter=\",\",fmt=\"%.5f\")\r\n",
    "#     np.savetxt(sim_qt_fout,combine_sim,delimiter=\",\",fmt=\"%.5f\")\r\n",
    "\r\n",
    "#     # 3.print log\r\n",
    "#     if (i+1) % 100 == 0:\r\n",
    "#         print((q_vstack - t_vstack).shape,combine_sim.shape)\r\n",
    "#     # # 4.gc\r\n",
    "#     # del df,q_vstack,t_vstack\r\n",
    "#     # gc.collect()\r\n",
    "\r\n",
    "# sentence_vec_fout.flush()\r\n",
    "# sim_qt_fout.flush()\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "2C24703EBA6A471E86B3D4A3FAC0581C",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79910\n80000\ntime: 1.23 s\n"
     ]
    }
   ],
   "source": [
    "!cat /home/kesci/work/similarity_feature/train_idf/consine_sentence_sim.csv | wc -l\n",
    "!cat /home/kesci/work/similarity_feature/train_idf/sentence_vector.csv | wc -l\n",
    "!cat /home/kesci/work/similarity_feature/train_idf/consine_sentence_sim.csv | head -n 1\n",
    "!cat /home/kesci/work/similarity_feature/train_idf/sentence_vector.csv | head -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0E634D9384374A56B3E47CC48270FB97"
   },
   "outputs": [],
   "source": [
    "########################  验证集 validation #######################\n",
    "########################  验证集 validation #######################\n",
    "########################  验证集 validation #######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "53C2238080AB4ACF90A69BF52F0982B0",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:37: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:55: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 128) (10000, 6)\n(10000, 128) (10000, 6)\n(10000, 128) (10000, 6)\n(10000, 128) (10000, 6)\n(10000, 128) (10000, 6)\ntime: 5h 4min 34s\n"
     ]
    }
   ],
   "source": [
    "############## 1000w-500w 验证集数据 ############\r\n",
    "word_dim = 128\r\n",
    "feature_path = \"/home/kesci/work/similarity_feature/valid_idf/\"\r\n",
    "train_data = \"/home/kesci/input/bytedance/first-round/train.csv\"\r\n",
    "# sentece vector of (query-title)\r\n",
    "sentence_vector_path = feature_path+\"sentence_vector.csv\"\r\n",
    "sentence_vec_fout = open(sentence_vector_path,\"w\")\r\n",
    "# 6dim similarity feature\r\n",
    "consine_feature_path = feature_path+\"consine_sentence_sim.csv\"\r\n",
    "sim_qt_fout = open(consine_feature_path,\"w\")\r\n",
    "\r\n",
    "samples = 100000000\r\n",
    "chunksize = 10000\r\n",
    "skip_num = int(samples/chunksize) - 1000\r\n",
    "print(skip_num)\r\n",
    "\r\n",
    "for i in range(int(500)):\r\n",
    "    df = pd.read_csv(train_data, names=colnames, header=None,skiprows=chunksize*(skip_num+i),nrows=chunksize,lineterminator=\"\\n\")\r\n",
    "    # 1.get sentece_vector & consine sim\r\n",
    "    q_vstack,t_vstack = concat_sentence(df)\r\n",
    "    combine_sim = cosine_sim(q_vstack,t_vstack)\r\n",
    "    # 2.save\r\n",
    "    np.savetxt(sentence_vec_fout,q_vstack - t_vstack,delimiter=\",\",fmt=\"%.5f\")\r\n",
    "    np.savetxt(sim_qt_fout,combine_sim,delimiter=\",\",fmt=\"%.5f\")\r\n",
    "\r\n",
    "    # 3.print log\r\n",
    "    if (i+1) % 100 == 0:\r\n",
    "        print((q_vstack - t_vstack).shape,combine_sim.shape)\r\n",
    "    # # 4.gc\r\n",
    "    # del df,q_vstack,t_vstack\r\n",
    "    # gc.collect()\r\n",
    "\r\n",
    "sentence_vec_fout.flush()\r\n",
    "sim_qt_fout.flush()\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "781A809439E646CB8186D9FE0A26B1AF"
   },
   "outputs": [],
   "source": [
    "!cat /home/kesci/work/similarity_feature/valid_idf/consine_sentence_sim.csv | wc -l\n",
    "!cat /home/kesci/work/similarity_feature/valid_idf/sentence_vector.csv | wc -l\n",
    "!cat /home/kesci/work/similarity_feature/valid_idf/consine_sentence_sim.csv | head -n 1\n",
    "!cat /home/kesci/work/similarity_feature/valid_idf/sentence_vector.csv | head -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C2C24767E9C24CDCBFE22161C0A7BCB3"
   },
   "outputs": [],
   "source": [
    "######################  测试集 Test ##########################\n",
    "######################  测试集 Test ##########################\n",
    "######################  测试集 Test #########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D43253D207B64BA18A1D65797AD48C70"
   },
   "outputs": [],
   "source": [
    "# ############## 测试集数据 ############\r\n",
    "# feature_path = \"/home/kesci/work/similarity_feature/test/\"\r\n",
    "# test_data = \"/home/kesci/input/bytedance/first-round/test.csv\"\r\n",
    "# # sentece vector of (query-title)\r\n",
    "# sentence_vector_path = feature_path+\"sentence_vector.csv\"\r\n",
    "# sentence_vec_fout = open(sentence_vector_path,\"w\")\r\n",
    "# # 6dim similarity feature\r\n",
    "# consine_feature_path = feature_path+\"consine_sentence_sim.csv\"\r\n",
    "# sim_qt_fout = open(consine_feature_path,\"w\")\r\n",
    "\r\n",
    "# chunksize = 10000\r\n",
    "\r\n",
    "# for i in range(int(500)):\r\n",
    "#     df = pd.read_csv(test_data, names=colnames, header=None,skiprows=chunksize*i,nrows=chunksize,lineterminator=\"\\n\")\r\n",
    "#     # 1.get sentece_vector & consine sim\r\n",
    "#     q_vstack,t_vstack = concat_sentence(df)\r\n",
    "#     combine_sim = cosine_sim(q_vstack,t_vstack)\r\n",
    "#     # 2.save\r\n",
    "#     np.savetxt(sentence_vec_fout,q_vstack - t_vstack,delimiter=\",\",fmt=\"%.5f\")\r\n",
    "#     np.savetxt(sim_qt_fout,combine_sim,delimiter=\",\",fmt=\"%.5f\")\r\n",
    "\r\n",
    "#     # 3.print log\r\n",
    "#     if (i+1) % 100 == 0:\r\n",
    "#         print((q_vstack - t_vstack).shape,combine_sim.shape)\r\n",
    "#     # # 4.gc\r\n",
    "#     # del df,q_vstack,t_vstack\r\n",
    "#     # gc.collect()\r\n",
    "\r\n",
    "# sentence_vec_fout.flush()\r\n",
    "# sim_qt_fout.flush()\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "58063A2C45F049A285E84723AB2B6CBC",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 596 µs\n"
     ]
    }
   ],
   "source": [
    "########################  query_word & sentence similarity 的计算 ###################\n",
    "########################          min,max,avg,median,std          ###################\n",
    "########################  query_word & sentence similarity 的计算 ###################\n",
    "########################          min,max,avg,median,std          ###################\n",
    "########################  query_word & sentence similarity 的计算 ###################\n",
    "########################          min,max,avg,median,std          ###################\n",
    "########################  query_word & sentence similarity 的计算 ###################\n",
    "########################          min,max,avg,median,std          ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6D51D030CFFB44669696F888CDEFF1EB",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.44 ms\n"
     ]
    }
   ],
   "source": [
    "##################################################\r\n",
    "##########计算word 和title_sentence sim\r\n",
    "##################################################\r\n",
    "word_dim = 128\r\n",
    "from scipy import spatial\r\n",
    "def cosine_sim(A, B):\r\n",
    "    return 1 - spatial.distance.cosine(A, B)\r\n",
    "\r\n",
    "def word_sentence_sim(origin_data):\r\n",
    "    sim_vstack = \"\"\r\n",
    "    for row in origin_data.values:        \r\n",
    "        # 1. title sentence vector\r\n",
    "        words = nltk.SpaceTokenizer().tokenize(row[3])\r\n",
    "        t_sentence_vec = np.full(word_dim,np.finfo(np.float32).eps)\r\n",
    "        M = []\r\n",
    "        for w in words:\r\n",
    "            try:\r\n",
    "                M.append(w2v_dict[w])\r\n",
    "            except:\r\n",
    "                continue\r\n",
    "        M = np.array(M)\r\n",
    "        if M.shape[0] != 0:\r\n",
    "            t_sentence_vec = M.sum(axis=0) / M.shape[0]\r\n",
    "        \r\n",
    "        # 2.get max,min,avg,median,std\r\n",
    "        words = nltk.SpaceTokenizer().tokenize(row[1])\r\n",
    "        M = []\r\n",
    "        for w in words:\r\n",
    "            try:\r\n",
    "                A = w2v_dict[w]\r\n",
    "                M.append(cosine_sim(A,t_sentence_vec))\r\n",
    "            except:\r\n",
    "                continue\r\n",
    "        M = np.array(M)\r\n",
    "        if M.shape[0] == 0:\r\n",
    "            M = np.zeros(word_dim)\r\n",
    "        sim_hstack = np.hstack(( \r\n",
    "                            np.min(M),np.max(M),\r\n",
    "                            np.average(M),np.median(M),np.std(M)\r\n",
    "                               ))         \r\n",
    "        if sim_vstack == \"\":\r\n",
    "            sim_vstack = sim_hstack.reshape(1,-1)\r\n",
    "        else:\r\n",
    "            sim_vstack = np.vstack((sim_vstack,sim_hstack.reshape(1,-1)))\r\n",
    "    return sim_vstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1A2B72D8D79148FFAACD1F05B2270539"
   },
   "outputs": [],
   "source": [
    "######################### 训练集数据 ########################\n",
    "######################### 训练集数据 ########################\n",
    "######################### 训练集数据 ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6C601E10454B4F0A82CDD5EDC9245FA7",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:41: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 5)\n(10000, 5)\n(10000, 5)\n(10000, 5)\n(10000, 5)\ntime: 1h 7min 9s\n"
     ]
    }
   ],
   "source": [
    "############## 后500w 训练集数据 ############\r\n",
    "feature_path = \"/home/kesci/work/similarity_feature/train/\"\r\n",
    "train_data = \"/home/kesci/input/bytedance/first-round/train.csv\"\r\n",
    "# # 5dim word_similarity feature\r\n",
    "consine_feature_path = feature_path+\"consine_word_sim.csv\"\r\n",
    "sim_qt_fout = open(consine_feature_path,\"w\")\r\n",
    "\r\n",
    "samples = 100000000\r\n",
    "chunksize = 10000\r\n",
    "skip_num = int(samples/chunksize) - 500\r\n",
    "print(skip_num)\r\n",
    "\r\n",
    "for i in range(int(500)):\r\n",
    "    df = pd.read_csv(train_data, names=colnames, header=None,skiprows=chunksize*i,nrows=chunksize,lineterminator=\"\\n\")\r\n",
    "    combine_sim = word_sentence_sim(df)\r\n",
    "    np.savetxt(sim_qt_fout,combine_sim,delimiter=\",\",fmt=\"%.5f\") \r\n",
    "    # print log\r\n",
    "    if (i+1) % 100 == 0:\r\n",
    "        print(combine_sim.shape)\r\n",
    "\r\n",
    "sim_qt_fout.flush()\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "640927F79D424CB695CCF748F2826D29"
   },
   "outputs": [],
   "source": [
    "######################### 验证集数据 ########################\n",
    "######################### 验证集数据 ########################\n",
    "######################### 验证集数据 ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8D89DC2BC3C041A7B5889916257AC766"
   },
   "outputs": [],
   "source": [
    "# ############## 1000w-500w 验证集数据 ############\r\n",
    "# feature_path = \"/home/kesci/work/similarity_feature/valid/\"\r\n",
    "# train_data = \"/home/kesci/input/bytedance/first-round/train.csv\"\r\n",
    "# # # 5dim word_similarity feature\r\n",
    "# consine_feature_path = feature_path+\"consine_sentence_sim.csv\"\r\n",
    "# sim_qt_fout = open(consine_feature_path,\"w\")\r\n",
    "\r\n",
    "# samples = 100000000\r\n",
    "# chunksize = 10000\r\n",
    "# skip_num = int(samples/chunksize) - 1000\r\n",
    "# print(skip_num)\r\n",
    "\r\n",
    "# for i in range(int(500)):\r\n",
    "#     df = pd.read_csv(train_data, names=colnames, header=None,skiprows=chunksize*i,nrows=chunksize,lineterminator=\"\\n\")\r\n",
    "#     combine_sim = word_sentence_sim(df)\r\n",
    "#     np.savetxt(sim_qt_fout,combine_sim,delimiter=\",\",fmt=\"%.5f\") \r\n",
    "#     # print log\r\n",
    "#     if (i+1) % 100 == 0:\r\n",
    "#         print(combine_sim.shape)\r\n",
    "\r\n",
    "# sim_qt_fout.flush()\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A49E12989B3C462799EFCFA86780EAD8"
   },
   "outputs": [],
   "source": [
    "######################### 测试集数据 ########################\n",
    "######################### 测试集数据 ########################\n",
    "######################### 测试集数据 ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "076B6B7D7ABA47018DD313403B0FFA96"
   },
   "outputs": [],
   "source": [
    "\r\n",
    "# feature_path = \"/home/kesci/work/similarity_feature/test/\"\r\n",
    "# test_data = \"/home/kesci/input/bytedance/first-round/test.csv\"\r\n",
    "# # 5dim word_similarity feature\r\n",
    "# consine_feature_path = feature_path+\"consine_word_sim.csv\"\r\n",
    "# sim_qt_fout = open(consine_feature_path,\"w\")\r\n",
    "\r\n",
    "# chunksize = 10000\r\n",
    "\r\n",
    "# for i in range(int(500)):\r\n",
    "#     df = pd.read_csv(test_data, names=colnames, header=None,skiprows=chunksize*i,nrows=chunksize,lineterminator=\"\\n\")\r\n",
    "#     combine_sim = word_sentence_sim(df)\r\n",
    "#     np.savetxt(sim_qt_fout,combine_sim,delimiter=\",\",fmt=\"%.5f\") \r\n",
    "#     # print log\r\n",
    "#     if (i+1) % 100 == 0:\r\n",
    "#         print(combine_sim.shape)\r\n",
    "\r\n",
    "# sim_qt_fout.flush()\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
